"""
AI è°ƒåº¦ä¸­å¿ƒ - åŒæ¨¡å‹ä¸¤é˜¶æ®µæ¨ç†æµç¨‹
"""
import asyncio
import time
from collections import deque
from typing import Optional, List, Dict, Any
from src.core.config_manager import ConfigManager
from src.core.logger import logger
from src.core.model_logger import get_model_logger
from src.services.http_client import AsyncHTTPClient
from src.models.api_types import ChatMessage


class AIManager:
    """
    AI è°ƒåº¦ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰
    
    åŒé˜¶æ®µæ¨ç†æµç¨‹ï¼š
    1. åœºæ™¯æ•´ç† (Organize Context): åˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œæå–å…³é”®ä¿¡æ¯
    2. å›å¤ç”Ÿæˆ (Generate Reply): åŸºäºåœºæ™¯æ‘˜è¦å’Œè§’è‰²è®¾å®šï¼Œç”Ÿæˆæœ€ç»ˆå›å¤
    """
    
    _instance: Optional['AIManager'] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self._initialized = True
        self.config = None
        # çŸ­æœŸå¯¹è¯å†…å­˜ï¼š{user_id: deque([(query, reply), ...])}
        self._short_term_memory: Dict[str, deque] = {}
        self._max_short_term_rounds = 100  # ç¼“å­˜æœ€å¤š 100 è½®å¯¹è¯ï¼ˆç”¨äºå­˜å‚¨ï¼‰
        self._bot_qq_id: Optional[str] = None  # Bot çš„ QQ å·ï¼Œç”¨äºè¯†åˆ«è‡ªå·±çš„æ¶ˆæ¯
        logger.info("âœ… AI Manager initialized (dual-stage reasoning mode)")
    
    async def load_history_from_napcat(self, bot, user_id: str, count: int = 200) -> int:
        """
        ä» NapCat åŠ è½½ç§èŠå†å²æ¶ˆæ¯åˆ°çŸ­æœŸå†…å­˜
        
        Args:
            bot: NoneBot Bot å®ä¾‹
            user_id: ç”¨æˆ· QQ å·
            count: æ‹‰å–æ¶ˆæ¯æ•°é‡ï¼ˆé»˜è®¤ 200 æ¡ï¼Œå°½å¯èƒ½å¤šåœ°åŠ è½½å†å²ï¼‰
            
        Returns:
            åŠ è½½çš„å¯¹è¯è½®æ•°
        """
        try:
            # è·å– Bot è‡ªå·±çš„ QQ å·
            if not self._bot_qq_id:
                bot_info = await bot.get_login_info()
                self._bot_qq_id = str(bot_info.get("user_id", ""))
            
            # è°ƒç”¨ NapCat API è·å–ç§èŠå†å²
            logger.debug(f"ğŸ“¥ è¯·æ±‚åŠ è½½ {count} æ¡å†å²æ¶ˆæ¯: user={user_id}")
            history = await bot.get_friend_msg_history(user_id=int(user_id), count=count)
            messages = history.get("messages", [])
            
            if not messages:
                logger.debug(f"   æœªè·å–åˆ°å†å²æ¶ˆæ¯")
                return 0
            
            logger.debug(f"   è·å–åˆ° {len(messages)} æ¡åŸå§‹æ¶ˆæ¯")
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
            messages.sort(key=lambda m: m.get("time", 0))
            
            # è§£ææ¶ˆæ¯ï¼Œé…å¯¹ Q&A
            pairs = []
            pending_query = None
            skipped_commands = 0
            skipped_empty = 0
            
            for msg in messages:
                sender_id = str(msg.get("sender", {}).get("user_id", ""))
                # æå–çº¯æ–‡æœ¬å†…å®¹
                text = ""
                for seg in msg.get("message", []):
                    if seg.get("type") == "text":
                        text += seg.get("data", {}).get("text", "")
                
                text = text.strip()
                if not text:
                    skipped_empty += 1
                    continue
                
                # è·³è¿‡å‘½ä»¤æ¶ˆæ¯ï¼ˆä»¥ / å¼€å¤´ï¼‰
                if text.startswith("/"):
                    pending_query = None  # é‡ç½®ï¼Œé¿å…å‘½ä»¤å›å¤è¢«é…å¯¹
                    skipped_commands += 1
                    continue
                
                if sender_id == self._bot_qq_id:
                    # Bot çš„æ¶ˆæ¯
                    if pending_query:
                        pairs.append((pending_query, text))
                        pending_query = None
                else:
                    # ç”¨æˆ·çš„æ¶ˆæ¯
                    # å¦‚æœæœ‰æœªé…å¯¹çš„æŸ¥è¯¢ï¼Œè¯´æ˜ç”¨æˆ·è¿ç»­å‘äº†å¤šæ¡ï¼Œåªä¿ç•™æœ€æ–°çš„
                    if pending_query:
                        logger.debug(f"   ç”¨æˆ·è¿ç»­æ¶ˆæ¯ï¼Œä¸¢å¼ƒ: {pending_query[:30]}")
                    pending_query = text
            
            logger.debug(f"   é…å¯¹ç»“æœ: {len(pairs)} è½®å¯¹è¯, è·³è¿‡å‘½ä»¤ {skipped_commands} æ¡, è·³è¿‡ç©ºæ¶ˆæ¯ {skipped_empty} æ¡")
            
            # å­˜å…¥çŸ­æœŸå†…å­˜
            if pairs:
                if user_id not in self._short_term_memory:
                    self._short_term_memory[user_id] = deque(maxlen=self._max_short_term_rounds)
                
                # åªå–æœ€è¿‘çš„ N è½®
                for query, reply in pairs[-self._max_short_term_rounds:]:
                    self._short_term_memory[user_id].append((query, reply))
                
                logger.info(f"ğŸ“¥ ä» NapCat åŠ è½½ {len(pairs)} è½®å†å²å¯¹è¯ï¼ˆå­˜å…¥ {min(len(pairs), self._max_short_term_rounds)} è½®ï¼‰: user={user_id}")
            
            return len(pairs)
            
        except Exception as e:
            logger.warning(f"ä» NapCat åŠ è½½å†å²å¤±è´¥: {e}")
            return 0
    
    async def load_group_history_from_napcat(self, bot, group_id: str, user_id: str, count: int = 300) -> int:
        """
        ä» NapCat åŠ è½½ç¾¤èŠå†å²æ¶ˆæ¯ï¼ˆç­›é€‰ç‰¹å®šç”¨æˆ·ï¼‰
        
        Args:
            bot: NoneBot Bot å®ä¾‹
            group_id: ç¾¤å·
            user_id: ç”¨æˆ· QQ å·
            count: æ‹‰å–æ¶ˆæ¯æ•°é‡ï¼ˆé»˜è®¤ 300 æ¡ï¼Œç¾¤èŠæ¶ˆæ¯å¤šï¼Œéœ€è¦æ›´å¤šæ‰èƒ½é…å¯¹å‡ºè¶³å¤Ÿçš„å¯¹è¯ï¼‰
            
        Returns:
            åŠ è½½çš„å¯¹è¯è½®æ•°
        """
        try:
            # è·å– Bot è‡ªå·±çš„ QQ å·
            if not self._bot_qq_id:
                bot_info = await bot.get_login_info()
                self._bot_qq_id = str(bot_info.get("user_id", ""))
            
            # è°ƒç”¨ NapCat API è·å–ç¾¤èŠå†å²
            logger.debug(f"ğŸ“¥ è¯·æ±‚åŠ è½½ {count} æ¡ç¾¤èŠå†å²: group={group_id}, user={user_id}")
            history = await bot.get_group_msg_history(group_id=int(group_id), count=count)
            messages = history.get("messages", [])
            
            if not messages:
                logger.debug(f"   æœªè·å–åˆ°å†å²æ¶ˆæ¯")
                return 0
            
            logger.debug(f"   è·å–åˆ° {len(messages)} æ¡åŸå§‹æ¶ˆæ¯")
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
            messages.sort(key=lambda m: m.get("time", 0))
            
            # è§£ææ¶ˆæ¯ï¼Œåªå…³æ³¨ç›®æ ‡ç”¨æˆ·å’Œ Bot çš„å¯¹è¯
            pairs = []
            pending_query = None
            skipped_other_users = 0
            skipped_commands = 0
            skipped_empty = 0
            
            for msg in messages:
                sender_id = str(msg.get("sender", {}).get("user_id", ""))
                
                # æå–çº¯æ–‡æœ¬å†…å®¹
                text = ""
                for seg in msg.get("message", []):
                    if seg.get("type") == "text":
                        text += seg.get("data", {}).get("text", "")
                
                text = text.strip()
                if not text:
                    skipped_empty += 1
                    continue
                
                # è·³è¿‡å‘½ä»¤æ¶ˆæ¯ï¼ˆä»¥ / å¼€å¤´ï¼‰
                if text.startswith("/"):
                    pending_query = None  # é‡ç½®ï¼Œé¿å…å‘½ä»¤å›å¤è¢«é…å¯¹
                    skipped_commands += 1
                    continue
                
                if sender_id == self._bot_qq_id:
                    # Bot çš„æ¶ˆæ¯ï¼Œå¦‚æœå‰é¢æœ‰è¯¥ç”¨æˆ·çš„æ¶ˆæ¯ï¼Œé…å¯¹
                    if pending_query:
                        pairs.append((pending_query, text))
                        pending_query = None
                elif sender_id == user_id:
                    # ç›®æ ‡ç”¨æˆ·çš„æ¶ˆæ¯
                    # å¦‚æœæœ‰æœªé…å¯¹çš„æŸ¥è¯¢ï¼Œè¯´æ˜ç”¨æˆ·è¿ç»­å‘äº†å¤šæ¡ï¼Œåªä¿ç•™æœ€æ–°çš„
                    if pending_query:
                        logger.debug(f"   ç”¨æˆ·è¿ç»­æ¶ˆæ¯ï¼Œä¸¢å¼ƒ: {pending_query[:30]}")
                    pending_query = text
                else:
                    # å…¶ä»–äººçš„æ¶ˆæ¯ï¼Œé‡ç½® pending
                    if pending_query:
                        skipped_other_users += 1
                    pending_query = None
            
            logger.debug(f"   é…å¯¹ç»“æœ: {len(pairs)} è½®å¯¹è¯, è·³è¿‡å…¶ä»–ç”¨æˆ· {skipped_other_users} æ¡, è·³è¿‡å‘½ä»¤ {skipped_commands} æ¡, è·³è¿‡ç©ºæ¶ˆæ¯ {skipped_empty} æ¡")
            
            # å­˜å…¥çŸ­æœŸå†…å­˜
            if pairs:
                if user_id not in self._short_term_memory:
                    self._short_term_memory[user_id] = deque(maxlen=self._max_short_term_rounds)
                
                # åªå–æœ€è¿‘çš„ N è½®
                for query, reply in pairs[-self._max_short_term_rounds:]:
                    self._short_term_memory[user_id].append((query, reply))
                
                logger.info(f"ğŸ“¥ ä» NapCat åŠ è½½ {len(pairs)} è½®ç¾¤èŠå†å²ï¼ˆå­˜å…¥ {min(len(pairs), self._max_short_term_rounds)} è½®ï¼‰: group={group_id}, user={user_id}")
            
            return len(pairs)
            
        except Exception as e:
            logger.warning(f"ä» NapCat åŠ è½½ç¾¤èŠå†å²å¤±è´¥: {e}")
            return 0
    
    def has_short_term_memory(self, user_id: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰çŸ­æœŸå†…å­˜"""
        return user_id in self._short_term_memory and len(self._short_term_memory[user_id]) > 0
    
    def _refresh_config(self) -> None:
        try:
            self.config = ConfigManager.get_ai_config()
        except RuntimeError:
            logger.warning("Config not loaded, please call ConfigManager.load()")
    
    async def chat(
        self,
        user_message: str,
        user_name: str = "ç”¨æˆ·",
        user_id: str = None,
        group_id: str = None,
        group_name: str = None
    ) -> str:
        """
        Handle chat request - dual-stage pipeline (å¢å¼ºç‰ˆï¼šæ”¯æŒ RAG + å¥½æ„Ÿåº¦)
        
        Args:
            user_message: User message
            user_name: User name (default: "ç”¨æˆ·")
            user_id: User ID (ç”¨äºæ£€ç´¢é•¿æœŸè®°å¿†å’Œå­˜å‚¨å¯¹è¯)
            group_id: Group ID (ç¾¤èŠæ—¶ä¼ å…¥)
            group_name: Group name (ç¾¤èŠæ—¶ä¼ å…¥)
            
        Returns:
            AI reply text
        """
        try:
            if self.config is None:
                self._refresh_config()
            
            # === è¾“å…¥æ¸…æ´—ï¼šæ£€æµ‹å¹¶è¿‡æ»¤æ³¨å…¥è¯æœ¯ ===
            from src.core.persona_guard import detect_injection, clean_injection
            is_injection, _ = detect_injection(user_message)
            if is_injection:
                user_message = clean_injection(user_message)
            
            # === é¢„å…ˆæ£€ç´¢çŸ¥è¯†åº“å’Œé•¿æœŸè®°å¿† ===
            from src.services.vector_service import get_vector_service
            vector_service = get_vector_service()
            
            # æ£€ç´¢çŸ¥è¯†åº“
            kb_info_raw = vector_service.search_knowledge(user_message)
            kb_stats = getattr(vector_service, '_last_kb_search_stats', {})
            
            # æ ¼å¼åŒ–çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆåŒ…å«æ£€ç´¢ç»Ÿè®¡ï¼Œç”¨äºæ—¥å¿—å’Œè°ƒè¯•ï¼‰
            if kb_info_raw:
                logger.info(f"ğŸ“š [çŸ¥è¯†åº“] å‘½ä¸­ {len(kb_info_raw)} å­—ç¬¦")
                logger.debug(f"   å†…å®¹é¢„è§ˆ: {kb_info_raw[:200]}...")
                # åœ¨çŸ¥è¯†åº“ä¿¡æ¯åé™„åŠ æ£€ç´¢ç»Ÿè®¡
                kb_info_with_stats = f"{kb_info_raw}\n\n[æ£€ç´¢ç»Ÿè®¡: æ•°æ®åº“æ€»æ•°={kb_stats.get('total_in_db', 0)}, æ£€ç´¢={kb_stats.get('fetched', 0)}æ¡, é€šè¿‡={kb_stats.get('passed', 0)}æ¡, è¿‡æ»¤={kb_stats.get('filtered', 0)}æ¡, é˜ˆå€¼={kb_stats.get('threshold', 0)}]"
            else:
                # å³ä½¿æ²¡æœ‰å‘½ä¸­ï¼Œä¹Ÿæ˜¾ç¤ºæ£€ç´¢ç»Ÿè®¡
                logger.info(f"ğŸ“š [çŸ¥è¯†åº“] æœªå‘½ä¸­")
                if 'skipped' in kb_stats:
                    kb_info_with_stats = f"ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰\n[æ£€ç´¢ç»Ÿè®¡: è·³è¿‡åŸå› ={kb_stats.get('skipped')}]"
                elif 'error' in kb_stats:
                    kb_info_with_stats = f"ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰\n[æ£€ç´¢ç»Ÿè®¡: é”™è¯¯={kb_stats.get('error')}]"
                else:
                    kb_info_with_stats = f"ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰\n[æ£€ç´¢ç»Ÿè®¡: æ•°æ®åº“æ€»æ•°={kb_stats.get('total_in_db', 0)}, æ£€ç´¢={kb_stats.get('fetched', 0)}æ¡, é€šè¿‡={kb_stats.get('passed', 0)}æ¡, è¿‡æ»¤={kb_stats.get('filtered', 0)}æ¡, é˜ˆå€¼={kb_stats.get('threshold', 0)}]"
            
            # æ£€ç´¢é•¿æœŸè®°å¿†ï¼ˆFAISS å‘é‡æ£€ç´¢ï¼‰
            long_mem = ""
            faiss_mem = ""
            if user_id:
                # ä¼ é€’ group_id ä»¥æ”¯æŒåœºæ™¯éš”ç¦»
                faiss_mem = vector_service.search_memory(
                    user_id, 
                    user_message,
                    group_id=group_id  # ä¼ é€’ç¾¤ID
                )
                if faiss_mem and faiss_mem != "ï¼ˆæš‚æ— ç›¸å…³é•¿æœŸè®°å¿†ï¼‰":
                    logger.info(f"ğŸ§  [FAISSå‘é‡] å‘½ä¸­ {len(faiss_mem)} å­—ç¬¦")
                    logger.debug(f"   å†…å®¹é¢„è§ˆ: {faiss_mem[:200]}...")
                    long_mem = faiss_mem
            
            # === æ£€ç´¢å…³ç³»å›¾è°±ï¼ˆRAG çŸ¥è¯†å›¾è°±ï¼‰===
            graph_mem = ""
            if user_id:
                try:
                    from src.core.RAGM import get_graph_retriever
                    graph_retriever = get_graph_retriever()
                    graph_mem = await graph_retriever.retrieve_with_graph(
                        user_id, user_message, user_name
                    )
                    if graph_mem:
                        logger.info(f"ğŸ•¸ï¸ [RAGå›¾è°±] å‘½ä¸­ {len(graph_mem)} å­—ç¬¦")
                        logger.debug(f"   å†…å®¹é¢„è§ˆ: {graph_mem[:200]}...")
                except Exception as e:
                    logger.warning(f"âš ï¸ RAGå›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            
            # åˆå¹¶ä¸¤ç§è®°å¿†æº
            if graph_mem:
                if long_mem:
                    # å°†å›¾è°±è®°å¿†ä½œä¸ºè¡¥å……ä¿¡æ¯æ·»åŠ 
                    long_mem = f"{long_mem}\n\nã€ç›¸å…³äº‹å®ã€‘{graph_mem}"
                    logger.info(f"âœ… [è®°å¿†åˆå¹¶] FAISS({len(faiss_mem)}å­—) + RAGå›¾è°±({len(graph_mem)}å­—) = æ€»è®¡{len(long_mem)}å­—")
                else:
                    # åªæœ‰å›¾è°±è®°å¿†
                    long_mem = f"ã€ç›¸å…³äº‹å®ã€‘{graph_mem}"
                    logger.info(f"âœ… [è®°å¿†æ¥æº] ä»…RAGå›¾è°± {len(graph_mem)}å­—")
            elif long_mem:
                logger.info(f"âœ… [è®°å¿†æ¥æº] ä»…FAISSå‘é‡ {len(long_mem)}å­—")
            
            # === è·å–å¥½æ„Ÿåº¦æ¸©åº¦ ===
            temperature = None
            if user_id:
                from src.core.Affection import get_affection_service
                affection_service = get_affection_service()
                default_temp = self.config.generator.temperature
                temperature = affection_service.get_temperature_for_user(user_id, default_temp)
                if temperature != default_temp:
                    logger.debug(f"ğŸ’• å¥½æ„Ÿåº¦æ¸©åº¦è°ƒæ•´: {default_temp} -> {temperature}")
            
            # === è·å–æœ€è¿‘å¯¹è¯ï¼ˆä»çŸ­æœŸå†…å­˜ï¼‰ ===
            # === è·å–æœ€è¿‘å¯¹è¯ï¼ˆä»çŸ­æœŸå†…å­˜ï¼‰ ===
            # ç¾¤èŠç”¨ group_id ä½œä¸º keyï¼Œç§èŠç”¨ user_id
            memory_key = group_id if group_id else user_id
            is_group = bool(group_id)
            
            # ä»é…ç½®è¯»å–å¯¹è¯è½®æ•°
            role_config = ConfigManager.get_role_config()
            dialogue_config = getattr(role_config, 'recent_dialogue', None)
            if dialogue_config:
                max_rounds = dialogue_config.group_max_rounds if is_group else dialogue_config.private_max_rounds
                max_chars = dialogue_config.max_chars
                logger.debug(f"ğŸ“ å¯¹è¯é…ç½®: max_rounds={max_rounds}, max_chars={max_chars}, is_group={is_group}")
            else:
                max_rounds = 4 if is_group else 6
                max_chars = 400
                logger.debug(f"ğŸ“ ä½¿ç”¨é»˜è®¤å¯¹è¯é…ç½®: max_rounds={max_rounds}, max_chars={max_chars}")
            
            recent_dialogue = self._get_recent_dialogue(memory_key, user_name, max_rounds=max_rounds, max_chars=max_chars, is_group=is_group)
            
            # Stage 1: Organize context (äº§å‡ºè®°å¿†æ‘˜è¦ï¼Œâ‰¤100å­—)
            # ç¾¤èŠå’Œç§èŠéƒ½éœ€è¦åœºæ™¯åˆ†æï¼Œä½†ç¾¤èŠæ—¶é•¿æœŸè®°å¿†ä¸ºç©º
            logger.info(f"ğŸ” Stage 1/3: Organizing context (memory summary)")
            context_summary = await self._organize_context(user_message, user_name, long_mem)
            logger.debug(f"   Memory summary: {context_summary[:100]}...")
            
            # === Stage 1.5: æ•´ç†çŸ¥è¯†åº“æ‘˜è¦ï¼ˆæ–°å¢ï¼‰===
            kb_summary = ""
            if kb_info_raw:
                logger.info(f"ğŸ“š Stage 1.5/3: Organizing knowledge base")
                logger.debug(f"   åŸå§‹çŸ¥è¯†åº“å†…å®¹: {kb_info_raw[:200]}...")
                # ä¼ å…¥åŸå§‹å†…å®¹ï¼ˆä¸å«æ£€ç´¢ç»Ÿè®¡ï¼‰ç»™ LLM æ•´ç†
                kb_summary = await self._organize_knowledge(user_message, kb_info_raw)
                logger.info(f"   æ•´ç†åæ‘˜è¦: {kb_summary[:100]}...")
                # åœ¨æ•´ç†åçš„æ‘˜è¦åé™„åŠ æ£€ç´¢ç»Ÿè®¡
                kb_summary_with_stats = f"{kb_summary}\n\n[æ£€ç´¢ç»Ÿè®¡: æ•°æ®åº“æ€»æ•°={kb_stats.get('total_in_db', 0)}, æ£€ç´¢={kb_stats.get('fetched', 0)}æ¡, é€šè¿‡={kb_stats.get('passed', 0)}æ¡, è¿‡æ»¤={kb_stats.get('filtered', 0)}æ¡, é˜ˆå€¼={kb_stats.get('threshold', 0)}]"
            else:
                logger.info(f"ğŸ“š Stage 1.5/3: è·³è¿‡ï¼ˆæ— çŸ¥è¯†åº“å†…å®¹ï¼‰")
                kb_summary_with_stats = kb_info_with_stats
            
            # Stage 2: Generate reply (æ–°ç‰ˆç»“æ„åŒ– prompt)
            logger.info(f"âœ¨ Stage 2/3: Generating reply (structured prompt)")
            final_reply = await self._generate_reply(
                context_summary, user_message, user_name, kb_summary_with_stats, 
                temperature_override=temperature,
                recent_dialogue=recent_dialogue,
                group_id=group_id,
                group_name=group_name,
                user_id=user_id
            )
            logger.debug(f"   Final reply: {final_reply[:100]}...")
            
            # === å›å¤å®ˆé—¨å‘˜ï¼šæ£€æŸ¥æ˜¯å¦è·‘å ===
            from src.core.persona_guard import check_reply_rules, check_reply_persona_match
            
            # 1. è§„åˆ™æ£€æŸ¥ï¼ˆé»‘åå•å…³é”®è¯ï¼‰
            rules_ok, violation = check_reply_rules(final_reply)
            
            # 2. äººè®¾å‘é‡ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå¯é€‰ï¼Œè¾ƒè€—æ—¶ï¼‰
            # persona_ok, similarity = await check_reply_persona_match(final_reply, threshold=0.45)
            
            # å¦‚æœè¿è§„ï¼Œè§¦å‘çº åé‡å†™
            if not rules_ok:
                logger.warning(f"ğŸ”„ è§¦å‘çº åé‡å†™: {violation}")
                final_reply = await self._correction_rewrite(
                    context_summary, user_message, user_name
                )
            
            # === å­˜å‚¨å¯¹è¯åˆ°çŸ­æœŸå†…å­˜ï¼ˆå®æ—¶ç”Ÿæ•ˆï¼‰===
            # ç¾¤èŠç”¨ group_id ä½œä¸º keyï¼Œç§èŠç”¨ user_id
            memory_key = group_id if group_id else user_id
            if memory_key:
                self._add_to_short_term_memory(memory_key, user_message, final_reply, sender_name=user_name)
            
            # === å­˜å‚¨å¯¹è¯åˆ°é•¿æœŸè®°å¿†ï¼ˆå‘é‡æ•°æ®åº“ï¼Œå¼‚æ­¥ï¼‰===
            if user_id:
                # ä¼ é€’ group_id ä»¥æ”¯æŒåŒæ•°æ®åº“å­˜å‚¨
                vector_service.add_pair_memory(
                    user_id, 
                    user_message, 
                    final_reply,
                    group_id=group_id,  # ä¼ é€’ç¾¤ID
                    sender_name=user_name
                )
            
            # === æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ–°å¢ï¼Œå¼‚æ­¥åå°ä»»åŠ¡ï¼‰===
            if user_id:
                try:
                    from src.core.RAGM import get_graph_retriever
                    graph_retriever = get_graph_retriever()
                    # åå°ä»»åŠ¡ï¼Œä¸é˜»å¡å“åº”
                    asyncio.create_task(
                        graph_retriever.add_dialogue_to_graph(
                            user_id, user_message, final_reply, user_name
                        )
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ å›¾è°±æ„å»ºä»»åŠ¡åˆ›å»ºå¤±è´¥: {e}")
            
            # === æ›´æ–°å¥½æ„Ÿåº¦ ===
            if user_id:
                from src.core.Affection import get_affection_service
                affection_service = get_affection_service()
                await affection_service.update_affection(user_id, user_message, final_reply)
            
            return final_reply
            
        except Exception as e:
            logger.error(f"âŒ Chat processing failed: {e}", exc_info=True)
            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
            logger.error(f"   ç”¨æˆ·: {user_id}, æ¶ˆæ¯: {user_message[:100]}")
            error_reply = self.config.fallback.error_reply if self.config else "An error occurred. Please try again."
            return error_reply
    
    async def _organize_context(
        self,
        user_message: str,
        user_name: str = "ç”¨æˆ·",
        long_mem: str = ""
    ) -> str:
        """
        Stage 1: ç”Ÿæˆè®°å¿†æ‘˜è¦ï¼ˆâ‰¤100å­—ï¼‰
        
        èŒè´£ï¼š
        - åŸºäºé•¿æœŸè®°å¿†ï¼Œæ¦‚æ‹¬æœˆä»£é›ªä¸ç”¨æˆ·ä¹‹é—´çš„é‡è¦äº’åŠ¨å’Œå…³ç³»ç‰¹å¾
        - è¾“å‡ºä¸€æ®µè¯ï¼Œä¸è¶…è¿‡100å­—
        - ä½¿ç”¨ç”¨æˆ·åæˆ–"å¯¹æ–¹"æŒ‡ä»£ï¼Œç¦æ­¢ä½¿ç”¨"ç”¨æˆ·"ä¸€è¯
        
        æ³¨æ„ï¼šçŸ¥è¯†åº“ä¿¡æ¯å’Œæœ€è¿‘å¯¹è¯ä¼šåœ¨ Stage 2 ç›´æ¥ä¼ é€’ç»™æ¨ç†æ¨¡å‹
        
        Args:
            user_message: Current user message
            user_name: Name of the user
            long_mem: é•¿æœŸè®°å¿†æ£€ç´¢ç»“æœ
            
        Returns:
            è®°å¿†æ‘˜è¦ï¼ˆâ‰¤100å­—ï¼‰
        """
        if not self.config:
            self._refresh_config()
        
        organizer = self.config.organizer
        
        if not organizer.enabled:
            logger.warning("Organizer model disabled, skipping stage 1")
            return f"ç”¨æˆ·è¾“å…¥ï¼š{user_message}"
        
        # === æ„å»º Organizer æç¤ºè¯ ===
        system_prompt = self._build_organizer_prompt()
        
        # å¦‚æœæœ‰é•¿æœŸè®°å¿†ï¼Œå°†å…¶ä½œä¸ºç³»ç»Ÿæç¤ºè¯çš„ä¸€éƒ¨åˆ†
        if long_mem and long_mem != "ï¼ˆæš‚æ— ç›¸å…³é•¿æœŸè®°å¿†ï¼‰":
            # æ ¼å¼åŒ–è®°å¿†å†…å®¹ï¼Œå°† "Useré—®" æ›¿æ¢ä¸ºç”¨æˆ·åï¼Œç§»é™¤ [Pair] æ ‡è®°
            formatted_mem = (
                long_mem
                .replace("[Pair] Useré—®:", f"{user_name}:")
                .replace("Useré—®:", f"{user_name}:")
                .replace("Botç­”:", "æœˆä»£é›ª:")
                .replace("[Pair] ", "")
            )
            
            # ä½¿ç”¨å ä½ç¬¦æ›¿æ¢è®°å¿†å†…å®¹
            memory_system_prompt = system_prompt.replace("{memory_content}", formatted_mem)
            
            user_prompt = (
                f"å¯¹è¯å¯¹è±¡: {user_name}\n"
                f"å½“å‰æ¶ˆæ¯: {user_message}\n\n"
                f"è¯·æ•´ç†ä¸Šè¿°å†å²è®°å¿†ã€‚"
            )
        else:
            # æ— è®°å¿†æ—¶çš„ç®€åŒ–å¤„ç†ï¼Œæ›¿æ¢å ä½ç¬¦ä¸ºæç¤ºæ–‡æœ¬
            memory_system_prompt = system_prompt.replace(
                "{memory_content}", 
                "(æš‚æ— å†å²è®°å¿†)"
            )
            user_prompt = (
                f"å¯¹è¯å¯¹è±¡: {user_name}\n"
                f"å½“å‰æ¶ˆæ¯: {user_message}\n\n"
                f"è¿™æ˜¯é¦–æ¬¡å¯¹è¯ï¼Œè¯·è¾“å‡º: é¦–æ¬¡å¯¹è¯ï¼Œæš‚æ— å†å²äº’åŠ¨"
            )
        
        messages = [
            ChatMessage(
                role="system",
                content=memory_system_prompt
            ),
            ChatMessage(
                role="user",
                content=user_prompt
            )
        ]
        
        try:
            start_time = time.time()
            response = await self._call_organizer_model(messages, organizer)
            summary = AsyncHTTPClient.parse_completion_response(response)
            elapsed_time = time.time() - start_time
            
            # è®°å½•æ¨¡å‹è°ƒç”¨
            if summary:
                model_logger = get_model_logger()
                model_logger.log_organizer_call(
                    user_message=user_message,
                    context_summary=summary,
                    system_prompt=memory_system_prompt,
                    model_name=organizer.model_name,
                    temperature=organizer.temperature,
                    max_tokens=organizer.max_tokens,
                    elapsed_time=elapsed_time
                )
            
            return summary if summary else f"User input: {user_message}"
            
        except Exception as e:
            logger.error(f"âŒ Context organization failed: {e}", exc_info=True)
            logger.error(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}")
            if self.config.fallback.skip_organizer_on_failure:
                logger.warning("   Skipping organizer, proceeding to reply generation")
                return f"User input: {user_message}"
            else:
                raise
    
    async def _organize_knowledge(
        self,
        user_message: str,
        kb_info: str
    ) -> str:
        """
        Stage 1.5: æ•´ç†çŸ¥è¯†åº“æ‘˜è¦
        
        èŒè´£ï¼š
        - ä»æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“ä¸­æå–ä¸å½“å‰å¯¹è¯ç›¸å…³çš„ä¿¡æ¯
        - å®¢è§‚ã€ç®€æ´åœ°æ•´ç†æˆæ‘˜è¦
        - è¾“å‡ºä¸è¶…è¿‡150å­—
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            kb_info: æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“ä¿¡æ¯
            
        Returns:
            çŸ¥è¯†åº“æ‘˜è¦ï¼ˆâ‰¤150å­—ï¼‰
        """
        if not self.config:
            self._refresh_config()
        
        # è·å–çŸ¥è¯†åº“æ•´ç†å™¨é…ç½®
        kb_organizer = getattr(self.config, 'kb_organizer', None)
        
        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"kb_organizer é…ç½®: {kb_organizer}")
        if kb_organizer:
            logger.debug(f"kb_organizer.enabled: {getattr(kb_organizer, 'enabled', None)}")
        
        # å¦‚æœæ²¡æœ‰é…ç½®æˆ–æœªå¯ç”¨ï¼Œç›´æ¥è¿”å›åŸå§‹å†…å®¹
        if not kb_organizer:
            logger.warning("âš ï¸ kb_organizer é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹çŸ¥è¯†åº“å†…å®¹")
            return kb_info
        
        if not getattr(kb_organizer, 'enabled', True):
            logger.warning("âš ï¸ kb_organizer æœªå¯ç”¨ï¼Œä½¿ç”¨åŸå§‹çŸ¥è¯†åº“å†…å®¹")
            return kb_info
        
        # è·å–æ¨¡å‹é…ç½®ï¼ˆå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨ organizer çš„é…ç½®ï¼‰
        provider_name = getattr(kb_organizer, 'provider', '') or getattr(self.config.organizer, 'provider', '')
        model_name = getattr(kb_organizer, 'model_name', '') or self.config.organizer.model_name
        temperature = getattr(kb_organizer, 'temperature', 0.2)
        max_tokens = getattr(kb_organizer, 'max_tokens', 300)
        timeout = getattr(kb_organizer, 'timeout', 60)
        
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = getattr(kb_organizer, 'system_prompt', None)
        if not system_prompt:
            system_prompt = """ä½ æ˜¯çŸ¥è¯†åº“æ•´ç†åŠ©æ‰‹ã€‚ä»æ£€ç´¢åˆ°çš„çŸ¥è¯†åº“ä¸­æå–ä¸ç”¨æˆ·æ¶ˆæ¯ç›¸å…³çš„ä¿¡æ¯ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
1. åªè¾“å‡ºä¸ç”¨æˆ·æ¶ˆæ¯ç›´æ¥ç›¸å…³çš„ä¿¡æ¯
2. å®¢è§‚ã€ç®€æ´ã€æ¸…æ™°ï¼Œä¸è¶…è¿‡150å­—
3. å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸ç”¨æˆ·æ¶ˆæ¯æ— å…³ï¼Œè¾“å‡º"æ— ç›¸å…³çŸ¥è¯†"
4. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹"""
        
        user_prompt = f"""ç”¨æˆ·æ¶ˆæ¯ï¼š{user_message}

çŸ¥è¯†åº“å†…å®¹ï¼š
{kb_info}

è¯·æ•´ç†å‡ºä¸ç”¨æˆ·æ¶ˆæ¯ç›¸å…³çš„çŸ¥è¯†ï¼ˆâ‰¤150å­—ï¼‰ï¼š"""
        
        messages = [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content=user_prompt)
        ]
        
        try:
            start_time = time.time()
            
            # è·å–ä¾›åº”å•†é…ç½®
            if not provider_name:
                provider_name = self.config.common.default_provider
            
            providers = getattr(self.config, 'providers', {})
            if provider_name in providers:
                provider = providers[provider_name]
                api_base = provider.api_base
                api_key = provider.api_key
                provider_timeout = provider.timeout
            else:
                raise ValueError(f"æœªæ‰¾åˆ°ä¾›åº”å•†é…ç½®: {provider_name}")
            
            # è°ƒç”¨æ¨¡å‹
            async with AsyncHTTPClient(timeout=timeout or provider_timeout) as client:
                response = await client.chat_completion(
                    api_base=api_base,
                    api_key=api_key,
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout or provider_timeout
                )
            
            summary = AsyncHTTPClient.parse_completion_response(response)
            elapsed_time = time.time() - start_time
            
            logger.info(f"ğŸ“š çŸ¥è¯†åº“æ•´ç†å®Œæˆ: {len(summary)}å­—, è€—æ—¶{elapsed_time:.2f}s")
            logger.debug(f"   æ•´ç†ç»“æœ: {summary[:100]}...")
            
            return summary if summary else kb_info
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ•´ç†å¤±è´¥: {e}")
            return kb_info
    
    async def _generate_reply(
        self,
        context_summary: str,
        user_message: str,
        user_name: str = "ç”¨æˆ·",
        kb_info: str = "",
        temperature_override: float = None,
        recent_dialogue: str = "",
        group_id: str = None,
        group_name: str = None,
        user_id: str = None
    ) -> str:
        """
        Stage 2: Reply generation (çŸ¥è¯†åº“åœ¨è¿™é‡Œç›´æ¥ä¼ é€’)
        
        Based on context summary, knowledge base, and persona, generate the final reply.
        
        Args:
            context_summary: Context from stage 1 (è®°å¿†æ‘˜è¦ï¼Œâ‰¤100å­—)
            user_message: Original user message
            user_name: Name of the user
            kb_info: çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼ˆå·²å‹ç¼©ä¸ºè¦ç‚¹å¥ï¼‰
            temperature_override: æ¸©åº¦è¦†ç›–å€¼ï¼ˆæ¥è‡ªå¥½æ„Ÿåº¦ç³»ç»Ÿï¼‰
            recent_dialogue: æœ€è¿‘å¯¹è¯è®°å½•
            group_id: ç¾¤å·ï¼ˆç¾¤èŠæ—¶ä¼ å…¥ï¼‰
            group_name: ç¾¤åï¼ˆç¾¤èŠæ—¶ä¼ å…¥ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºè·å–å¥½æ„Ÿåº¦ï¼‰
            
        Returns:
            AI reply
        """
        if not self.config:
            self._refresh_config()
        
        generator = self.config.generator
        
        if not generator.enabled:
            logger.error("Generator model disabled")
            raise RuntimeError("Generator model not enabled")
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆåŒºåˆ†ç§èŠ/ç¾¤èŠæ¨¡æ¿ï¼‰
        system_prompt = self._build_system_prompt(
            context_summary, user_name, kb_info, recent_dialogue,
            group_id=group_id, group_name=group_name, user_id=user_id
        )
        
        messages = [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content=user_message)
        ]
        
        # ä½¿ç”¨å¥½æ„Ÿåº¦æ¸©åº¦æˆ–é»˜è®¤æ¸©åº¦
        actual_temperature = temperature_override if temperature_override is not None else generator.temperature
        
        try:
            start_time = time.time()
            response = await self._call_generator_model(messages, generator, actual_temperature)
            reply = AsyncHTTPClient.parse_completion_response(response)
            reasoning = AsyncHTTPClient.parse_reasoning_content(response)  # æå–æ€è€ƒè¿‡ç¨‹
            elapsed_time = time.time() - start_time
            
            # === åå¤„ç†ï¼šå¼ºåˆ¶ç§»é™¤æ‹¬å·å†…å®¹ï¼ˆå…œåº•ï¼‰ ===
            if reply:
                import re
                # ç§»é™¤æ‰€æœ‰æ‹¬å·åŠå…¶å†…å®¹ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡æ‹¬å·ï¼‰
                reply = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', reply)
                reply = re.sub(r'[ã€\[].*?[ã€‘\]]', '', reply)
                reply = re.sub(r'[ã€Š<].*?[ã€‹>]', '', reply)
                # ç§»é™¤æ‰€æœ‰å¥å·
                reply = reply.replace('ã€‚', '')
                # æ¸…ç†å¤šä½™ç©ºæ ¼
                reply = re.sub(r'\s+', ' ', reply).strip()
                # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œç”¨çœç•¥å·å…œåº•
                if not reply or len(reply) < 2:
                    reply = "......"
            
            # è®°å½•æ¨¡å‹è°ƒç”¨ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
            if reply:
                model_logger = get_model_logger()
                model_logger.log_generator_call(
                    user_message=user_message,
                    context_summary=context_summary,
                    system_prompt=system_prompt,
                    reply=reply,
                    model_name=generator.model_name,
                    temperature=actual_temperature,
                    max_tokens=generator.max_tokens,
                    elapsed_time=elapsed_time,
                    reasoning_content=reasoning  # ä¼ é€’æ€è€ƒè¿‡ç¨‹
                )
            
            return reply if reply else self.config.fallback.error_reply
            
        except Exception as e:
            logger.error(f"Reply generation failed: {e}", exc_info=True)
            logger.error(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message[:100]}")
            logger.error(f"   ä¸Šä¸‹æ–‡æ‘˜è¦: {context_summary[:100]}")
            raise
    
    def _format_chat_history(self, history: List[ChatMessage]) -> str:
        """Format chat history for readability"""
        if not history:
            return "(No chat history)"
        
        lines = []
        for msg in history:
            if msg.role == "user":
                lines.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                lines.append(f"Yuki: {msg.content}")
        
        return "\n".join(lines[-10:])
    
    def _get_recent_dialogue(
        self, 
        memory_key: str, 
        user_name: str, 
        max_rounds: int = 6,
        max_chars: int = 400,
        is_group: bool = False
    ) -> str:
        """
        è·å–æœ€è¿‘å¯¹è¯è®°å½•ï¼ˆä»çŸ­æœŸå†…å­˜ï¼‰
        
        ç§èŠæ ¼å¼ï¼š
        {user_name}ï¼šâ€¦â€¦
        æœˆä»£é›ªï¼šâ€¦â€¦
        
        ç¾¤èŠæ ¼å¼ï¼š
        {sender_name}ï¼šâ€¦â€¦
        æœˆä»£é›ªï¼šâ€¦â€¦
        
        Args:
            memory_key: å†…å­˜ keyï¼ˆç§èŠç”¨ user_idï¼Œç¾¤èŠç”¨ group_idï¼‰
            user_name: å½“å‰ç”¨æˆ·æ˜µç§°ï¼ˆç§èŠæ—¶ä½¿ç”¨ï¼‰
            max_rounds: æœ€å¤§è½®æ•°
            max_chars: æœ€å¤§å­—ç¬¦æ•°
            is_group: æ˜¯å¦ç¾¤èŠ
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å­—ç¬¦ä¸²
        """
        try:
            # ä»çŸ­æœŸå†…å­˜è·å–
            if memory_key not in self._short_term_memory:
                return ""
            
            pairs = list(self._short_term_memory[memory_key])
            if not pairs:
                return ""
            
            # æ ¼å¼åŒ–è¾“å‡ºï¼Œä¼˜å…ˆä¿è¯è½®æ•°
            lines = []
            role_name = ConfigManager.get_role_config().persona.name
            
            # ä»æ—§åˆ°æ–°éå†ï¼Œå–æœ€è¿‘ max_rounds è½®
            for item in pairs[-max_rounds:]:
                # å…¼å®¹æ—§æ ¼å¼ (query, reply) å’Œæ–°æ ¼å¼ (query, reply, sender_name)
                if len(item) == 3:
                    query, reply, sender_name = item
                else:
                    query, reply = item
                    sender_name = user_name  # ç§èŠæˆ–æ—§æ•°æ®ç”¨å½“å‰ç”¨æˆ·å
                
                # ç¾¤èŠæ˜¾ç¤ºå‘é€è€…åå­—ï¼Œç§èŠç»Ÿä¸€ç”¨ user_name
                display_name = sender_name if is_group else user_name
                line = f"{display_name}ï¼š{query}\n{role_name}ï¼š{reply}"
                lines.append(line)
            
            # æ‹¼æ¥æ‰€æœ‰å¯¹è¯
            result = "\n".join(lines)
            
            # å¦‚æœè¶…è¿‡å­—ç¬¦é™åˆ¶ï¼Œä»å‰é¢æˆªæ–­ï¼ˆä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼‰
            if len(result) > max_chars:
                # ä»åå¾€å‰ç´¯åŠ ï¼Œä¿è¯æœ€è¿‘çš„å¯¹è¯ä¸è¢«æˆªæ–­
                truncated_lines = []
                total_chars = 0
                for line in reversed(lines):
                    if total_chars + len(line) + 1 > max_chars:  # +1 for newline
                        break
                    truncated_lines.insert(0, line)
                    total_chars += len(line) + 1
                result = "\n".join(truncated_lines)
                logger.debug(f"å¯¹è¯è®°å½•è¶…é•¿ï¼Œæˆªæ–­ä¸º {len(truncated_lines)} è½®ï¼ˆ{total_chars}å­—ï¼‰")
            
            return result
            
        except Exception as e:
            logger.warning(f"è·å–æœ€è¿‘å¯¹è¯å¤±è´¥: {e}")
            return ""
    
    def _add_to_short_term_memory(self, memory_key: str, query: str, reply: str, sender_name: str = None) -> None:
        """
        æ·»åŠ å¯¹è¯åˆ°çŸ­æœŸå†…å­˜
        
        Args:
            memory_key: å†…å­˜ keyï¼ˆç§èŠç”¨ user_idï¼Œç¾¤èŠç”¨ group_idï¼‰
            query: ç”¨æˆ·æ¶ˆæ¯
            reply: Bot å›å¤
            sender_name: å‘é€è€…æ˜µç§°ï¼ˆç¾¤èŠæ—¶ä½¿ç”¨ï¼‰
        """
        if memory_key not in self._short_term_memory:
            self._short_term_memory[memory_key] = deque(maxlen=self._max_short_term_rounds)
        
        # å­˜å‚¨æ ¼å¼ï¼š(query, reply, sender_name)
        self._short_term_memory[memory_key].append((query, reply, sender_name or "ç”¨æˆ·"))
    
    def _compress_kb_info(self, kb_info: str, max_items: int = 3) -> str:
        """
        å‹ç¼©çŸ¥è¯†åº“ä¿¡æ¯ä¸ºè¦ç‚¹å¥
        
        å°†æ£€ç´¢åˆ°çš„åŸæ–‡å‹ç¼©æˆæ¯æ¡ 50-80 å­—çš„è¦ç‚¹å¥ï¼Œä¿ç•™å®Œæ•´è¯­ä¹‰
        
        Args:
            kb_info: åŸå§‹çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
            max_items: æœ€å¤§æ¡ç›®æ•°
            
        Returns:
            å‹ç¼©åçš„çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆç¼–å·åˆ—è¡¨ï¼‰
        """
        if not kb_info or kb_info == "ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰":
            return "ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰"
        
        try:
            # è§£ææ£€ç´¢ç»“æœï¼ˆæŒ‰æ¡ç›®åˆ†å‰²ï¼‰
            lines = kb_info.strip().split('\n')
            compressed_items = []
            current_item = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æµ‹æ–°æ¡ç›®å¼€å§‹ï¼ˆä»¥æ•°å­—+ç‚¹å¼€å¤´ï¼‰
                if line and line[0].isdigit() and '.' in line[:3]:
                    if current_item:
                        # å¤„ç†ä¸Šä¸€æ¡
                        item_text = ' '.join(current_item)
                        compressed = self._extract_key_sentence(item_text)
                        if compressed:
                            compressed_items.append(compressed)
                    current_item = [line]
                else:
                    current_item.append(line)
            
            # å¤„ç†æœ€åä¸€æ¡
            if current_item:
                item_text = ' '.join(current_item)
                compressed = self._extract_key_sentence(item_text)
                if compressed:
                    compressed_items.append(compressed)
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥è¿”å›åŸæ–‡ï¼ˆä¸æˆªæ–­ï¼‰
            if not compressed_items:
                return kb_info
            
            # æ ¼å¼åŒ–è¾“å‡ºï¼ˆé‡æ–°ç¼–å·ï¼Œé¿å…é‡å¤ï¼‰
            result_lines = []
            for i, item in enumerate(compressed_items[:max_items], 1):
                result_lines.append(f"{i}. {item}")
            
            return "\n".join(result_lines)
            
        except Exception as e:
            logger.warning(f"å‹ç¼©çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {e}")
            return kb_info
    
    def _extract_key_sentence(self, text: str, max_len: int = 80) -> str:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®å¥ï¼ˆ50-80å­—ï¼‰ï¼Œä¿ç•™å®Œæ•´è¯­ä¹‰
        
        ç­–ç•¥ï¼šç§»é™¤æ ¼å¼æ ‡è®°å’ŒåŸæœ‰ç¼–å·ï¼Œå–å®Œæ•´çš„å‰1-2å¥
        """
        if not text:
            return ""
        
        import re
        
        # ç§»é™¤æ ¼å¼æ ‡è®°
        text = text.replace("æ ‡é¢˜ï¼š", "").replace("å†…å®¹ï¼š", "").replace("ç›¸å…³æ€§ï¼š", "")
        text = text.replace("æœç´¢ç±»å‹ï¼švector", "").replace("æœç´¢ç±»å‹ï¼škeyword", "")
        
        # ç§»é™¤å¼€å¤´çš„ç¼–å·ï¼ˆå¦‚ "1. " "1ã€‚" "2. " ç­‰ï¼‰
        text = re.sub(r'^[\d]+[.ã€‚]\s*', '', text.strip())
        
        # ç§»é™¤æ¥æºæ ‡è®°ï¼ˆå¦‚ "é­”å¥³å®¡åˆ¤çŸ¥è¯†åº“ï¼š" "é­”è£è®¾å®šï¼š"ï¼‰
        text = re.sub(r'^[^ï¼š:]+[ï¼š:]\s*', '', text, count=1)
        
        # æŒ‰å¥å·åˆ†å‰²ï¼Œä¿ç•™å®Œæ•´å¥å­
        sentences = []
        # ç”¨æ­£åˆ™åˆ†å‰²ï¼Œä¿ç•™åˆ†éš”ç¬¦
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        
        # é‡ç»„å¥å­ï¼ˆå†…å®¹+æ ‡ç‚¹ï¼‰
        i = 0
        while i < len(parts):
            sentence = parts[i].strip()
            if i + 1 < len(parts) and parts[i + 1] in 'ã€‚ï¼ï¼Ÿ':
                sentence += parts[i + 1]
                i += 2
            else:
                i += 1
            if sentence:
                sentences.append(sentence)
        
        if not sentences:
            # æ²¡æœ‰å¥å·ï¼Œç›´æ¥ç”¨åŸæ–‡
            result = text.strip()
        else:
            # å–ç¬¬ä¸€å¥
            result = sentences[0]
            
            # å¦‚æœå¤ªçŸ­ï¼ˆ<30å­—ï¼‰ä¸”æœ‰ç¬¬äºŒå¥ï¼Œæ‹¼æ¥
            if len(result) < 30 and len(sentences) > 1:
                result = result + sentences[1]
        
        # å¦‚æœè¶…é•¿ï¼Œåœ¨å¥å·å¤„æˆªæ–­ï¼ˆè€Œä¸æ˜¯ç¡¬æˆªæ–­ï¼‰
        if len(result) > max_len:
            # æ‰¾æœ€åä¸€ä¸ªå¥å·ä½ç½®
            for sep in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ']:
                last_sep = result[:max_len].rfind(sep)
                if last_sep > 30:  # è‡³å°‘ä¿ç•™30å­—
                    result = result[:last_sep + 1]
                    break
            else:
                # å®åœ¨æ‰¾ä¸åˆ°ï¼Œç¡¬æˆªæ–­ä½†ä¸åŠ çœç•¥å·ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±æ„Ÿï¼‰
                result = result[:max_len]
        
        return result
        
        return result
    
    def _build_organizer_prompt(self) -> str:
        """Build organizer model system prompt - no user info needed in this stage"""
        organizer_config = self.config.organizer
        
        # æ„å»ºæç¤ºè¯ï¼ˆæ­¤é˜¶æ®µä¸éœ€è¦å¡«å……ç”¨æˆ·åå’Œæ—¶é—´ï¼‰
        prompt_template = organizer_config.system_prompt
        
        if not prompt_template:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯
            return "åˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œæå–æ„å›¾ã€ä¸»é¢˜ã€å…³é”®ä¿¡æ¯å’Œåº”å¯¹æ€åº¦ã€‚ä¸è¦ç”Ÿæˆå›å¤ã€‚"
        
        return prompt_template
    

    
    def _build_system_prompt(
        self, 
        context_summary: str, 
        user_name: str = "ç”¨æˆ·",
        kb_info: str = "",
        recent_dialogue: str = "",
        group_id: str = None,
        group_name: str = None,
        user_id: str = None
    ) -> str:
        """
        Build complete system prompt - åŒºåˆ†ç§èŠ/ç¾¤èŠæ¨¡æ¿
        
        Args:
            context_summary: Organizer äº§å‡ºçš„è®°å¿†æ‘˜è¦ï¼ˆâ‰¤100å­—ï¼‰
            user_name: ç”¨æˆ·å
            kb_info: çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼ˆå·²å‹ç¼©ä¸ºè¦ç‚¹å¥ï¼‰
            recent_dialogue: æœ€è¿‘å¯¹è¯è®°å½•
            group_id: ç¾¤å·ï¼ˆç¾¤èŠæ—¶ä¼ å…¥ï¼‰
            group_name: ç¾¤åï¼ˆç¾¤èŠæ—¶ä¼ å…¥ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºè·å–å¥½æ„Ÿåº¦ï¼‰
        """
        from datetime import datetime
        
        role_config = ConfigManager.get_role_config()
        
        # æ ¹æ®æ˜¯å¦ç¾¤èŠé€‰æ‹©æ¨¡æ¿
        is_group = bool(group_id)
        if is_group:
            template = getattr(role_config.system_prompt_template, 'group_template', None)
            if not template:
                # å¦‚æœæ²¡æœ‰ç¾¤èŠæ¨¡æ¿ï¼Œç”¨ç§èŠæ¨¡æ¿
                template = role_config.system_prompt_template.template
        else:
            template = role_config.system_prompt_template.template
        
        # è§’è‰²æ ¸å¿ƒè®¾å®šï¼ˆå†™æ­»åœ¨é…ç½®é‡Œï¼‰
        role_profile = getattr(role_config.system_prompt_template, 'role_profile', '') or role_config.expression.description
        
        # è¯­è¨€é£æ ¼
        expression_style = role_config.expression.speaking_style or "ç†æ€§ã€å†·æ¼ ï¼Œè¯´è¯å¹³æ·¡å…‹åˆ¶"
        
        # è§„åˆ™ï¼ˆæ”¯æŒ {user_name} å ä½ç¬¦ï¼‰
        conversation_rules = role_config.system_prompt_template.conversation_rules
        if conversation_rules:
            conversation_rules = conversation_rules.replace("{user_name}", user_name)
        
        # å½“å‰æ—¶é—´
        current_datetime = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        
        # è®°å¿†æ‘˜è¦ï¼ˆæ¥è‡ª context_summaryï¼Œå¦‚æœä¸ºç©ºåˆ™æ˜¾ç¤ºé»˜è®¤ï¼‰
        memory_summary = context_summary.strip() if context_summary else "æš‚æ— é•¿æœŸè®°å¿†"
        
        # æœ€è¿‘å¯¹è¯ï¼ˆå¦‚æœä¸ºç©ºåˆ™æ˜¾ç¤ºé»˜è®¤ï¼‰
        if not recent_dialogue:
            recent_dialogue = "ï¼ˆæš‚æ— æœ€è¿‘å¯¹è¯ï¼‰"
        
        # çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆå¦‚æœä¸ºç©ºåˆ™æ˜¾ç¤ºé»˜è®¤ï¼‰
        if not kb_info:
            kb_info = "ï¼ˆæ— ç›¸å…³çŸ¥è¯†ï¼‰"
        
        # ç¾¤åï¼ˆå¦‚æœä¸ºç©ºåˆ™ç”¨ç¾¤å·ï¼‰
        display_group_name = group_name or group_id or ""
        
        # è·å–å¥½æ„Ÿåº¦ä¿¡æ¯ï¼ˆç§èŠå’Œç¾¤èŠéƒ½è·å–ä¸ªäººå¥½æ„Ÿåº¦ï¼‰
        affection_level = "æœªçŸ¥"
        if user_id:
            try:
                from src.core.Affection import get_affection_service
                affection_service = get_affection_service()
                info = affection_service.get_affection_info_for_display(user_id)
                affection_level = f"{info['level_name']}ï¼ˆ{info['score']}/10ï¼‰"
            except Exception:
                affection_level = "æœªçŸ¥"
        
        # å¡«å……æ¨¡æ¿ï¼ˆå…¼å®¹ç§èŠå’Œç¾¤èŠï¼‰
        try:
            system_prompt = template.format(
                role_profile=role_profile,
                expression_style=expression_style,
                current_datetime=current_datetime,
                user_name=user_name,
                memory_summary=memory_summary,
                recent_dialogue=recent_dialogue,
                kb_info=kb_info,
                conversation_rules=conversation_rules,
                group_name=display_group_name,  # ç¾¤èŠæ¨¡æ¿ç”¨
                affection_level=affection_level  # å¥½æ„Ÿåº¦
            )
        except KeyError:
            # å¦‚æœæ¨¡æ¿ç¼ºå°‘æŸäº›å ä½ç¬¦ï¼Œç”¨ç§èŠæ¨¡æ¿å…œåº•
            system_prompt = role_config.system_prompt_template.template.format(
                role_profile=role_profile,
                expression_style=expression_style,
                current_datetime=current_datetime,
                user_name=user_name,
                memory_summary=memory_summary,
                recent_dialogue=recent_dialogue,
                kb_info=kb_info,
                conversation_rules=conversation_rules,
                affection_level=affection_level
            )
        
        return system_prompt
    
    def _get_provider_config(self, provider_name: str = None):
        """
        è·å–ä¾›åº”å•†é…ç½®
        
        Args:
            provider_name: ä¾›åº”å•†åç§°ï¼Œä¸ºç©ºåˆ™ç”¨é»˜è®¤ä¾›åº”å•†
            
        Returns:
            (api_base, api_key, timeout)
        """
        # ç¡®å®šä½¿ç”¨å“ªä¸ªä¾›åº”å•†
        if not provider_name:
            provider_name = self.config.common.default_provider
        
        # ä» providers å­—å…¸è·å–
        providers = getattr(self.config, 'providers', {})
        if provider_name in providers:
            provider = providers[provider_name]
            return provider.api_base, provider.api_key, provider.timeout
        
        # å…¼å®¹æ—§é…ç½®ï¼šå¦‚æœæ²¡æœ‰ providersï¼Œç”¨ common é‡Œçš„
        if hasattr(self.config.common, 'api_base') and self.config.common.api_base:
            return self.config.common.api_base, self.config.common.api_key, self.config.common.timeout
        
        raise ValueError(f"æœªæ‰¾åˆ°ä¾›åº”å•†é…ç½®: {provider_name}")
    
    async def _call_organizer_model(
        self,
        messages: List[ChatMessage],
        organizer_config
    ) -> Dict[str, Any]:
        """Call organizer model"""
        # è·å–ä¾›åº”å•†é…ç½®
        provider_name = getattr(organizer_config, 'provider', '') or None
        api_base, api_key, provider_timeout = self._get_provider_config(provider_name)
        timeout = organizer_config.timeout or provider_timeout
        
        async with AsyncHTTPClient(timeout=timeout) as client:
            response = await client.chat_completion(
                api_base=api_base,
                api_key=api_key,
                model=organizer_config.model_name,
                messages=messages,
                temperature=organizer_config.temperature,
                max_tokens=organizer_config.max_tokens,
                timeout=timeout
            )
            
            # è®°å½• LLM ä½¿ç”¨ç»Ÿè®¡
            self._record_llm_stats(organizer_config.model_name, response)
            
            return response
    
    async def _call_generator_model(
        self,
        messages: List[ChatMessage],
        generator_config,
        temperature: float = None
    ) -> Dict[str, Any]:
        """Call generator model"""
        # è·å–ä¾›åº”å•†é…ç½®
        provider_name = getattr(generator_config, 'provider', '') or None
        api_base, api_key, provider_timeout = self._get_provider_config(provider_name)
        timeout = generator_config.timeout or provider_timeout
        
        # ä½¿ç”¨ä¼ å…¥çš„æ¸©åº¦æˆ–é…ç½®çš„é»˜è®¤æ¸©åº¦
        actual_temp = temperature if temperature is not None else generator_config.temperature
        
        async with AsyncHTTPClient(timeout=timeout) as client:
            response = await client.chat_completion(
                api_base=api_base,
                api_key=api_key,
                model=generator_config.model_name,
                messages=messages,
                temperature=actual_temp,
                max_tokens=generator_config.max_tokens,
                timeout=timeout
            )
            
            # è®°å½• LLM ä½¿ç”¨ç»Ÿè®¡
            self._record_llm_stats(generator_config.model_name, response)
            
            return response
    
    def _record_llm_stats(self, model_name: str, response: Dict[str, Any]) -> None:
        """è®°å½• LLM ä½¿ç”¨ç»Ÿè®¡"""
        try:
            from src.services.stats_service import get_stats_service
            usage = AsyncHTTPClient.parse_usage(response)
            
            if usage["prompt_tokens"] > 0 or usage["completion_tokens"] > 0:
                stats_service = get_stats_service()
                stats_service.record_llm_usage(
                    model_name=model_name,
                    input_tokens=usage["prompt_tokens"],
                    output_tokens=usage["completion_tokens"]
                )
        except Exception as e:
            logger.warning(f"è®°å½• LLM ç»Ÿè®¡å¤±è´¥: {e}")
    
    async def _correction_rewrite(
        self,
        context_summary: str,
        user_message: str,
        user_name: str
    ) -> str:
        """
        çº åé‡å†™ï¼šå½“å›å¤è·‘åæ—¶ï¼Œç”¨ç²¾ç®€ prompt é‡æ–°ç”Ÿæˆ
        
        åªä¼ æœ€æ ¸å¿ƒçš„äººè®¾é”šç‚¹ï¼Œä¸ä¼ çŸ¥è¯†åº“ç­‰é™„åŠ ä¿¡æ¯
        """
        if not self.config:
            self._refresh_config()
        
        generator = self.config.generator
        role_config = ConfigManager.get_role_config()
        
        # ç²¾ç®€çš„çº å prompt
        correction_prompt = f"""ä½ æ˜¯æœˆä»£é›ªï¼Œé­”å¥³ç§æ—æœ€åçš„å¹¸å­˜è€…ã€‚è¯´è¯å†·æ·¡ç®€çŸ­ï¼Œ1-2å¥è¯ã€‚

ä¸Šä¸€æ¬¡å›å¤ä¸ç¬¦åˆè§’è‰²è®¾å®šã€‚è¯·é‡æ–°å›å¤ä¸‹é¢çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œä¸¥æ ¼ä¿æŒè§’è‰²ã€‚
ç¦æ­¢è¯´"ä½œä¸ºAI"æˆ–è®¨è®ºè§„åˆ™æœ¬èº«ã€‚

åœºæ™¯æ¦‚æ‹¬ï¼š{context_summary[:200]}
ç”¨æˆ·ï¼ˆ{user_name}ï¼‰è¯´ï¼š{user_message}"""
        
        messages = [
            ChatMessage(role="user", content=correction_prompt)
        ]
        
        try:
            response = await self._call_generator_model(
                messages, generator, temperature=0.5  # é™ä½æ¸©åº¦å¢åŠ ç¨³å®šæ€§
            )
            reply = AsyncHTTPClient.parse_completion_response(response)
            logger.info(f"ğŸ”„ çº åé‡å†™å®Œæˆ: {reply[:50]}...")
            return reply if reply else self.config.fallback.error_reply
        except Exception as e:
            logger.error(f"âŒ çº åé‡å†™å¤±è´¥: {e}")
            return "......"  # æœ€ç®€å…œåº•


_ai_manager: Optional[AIManager] = None


def get_ai_manager() -> AIManager:
    """Get global AI Manager singleton"""
    global _ai_manager
    if _ai_manager is None:
        _ai_manager = AIManager()
    return _ai_manager
