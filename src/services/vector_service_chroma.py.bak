"""
å‘é‡æœåŠ¡ - é›†æˆ ChromaDB å®ç°é•¿æœŸè®°å¿†å­˜å‚¨å’Œæ£€ç´¢
è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶åœ¨å‘é‡æ•°æ®åº“ä¸­å­˜å‚¨å’ŒæŸ¥è¯¢

v2.0 æ›´æ–°ï¼š
- é›†æˆå¤šå±‚æ¬¡æ£€ç´¢ç­–ç•¥
- æ”¯æŒåœºæ™¯æ„ŸçŸ¥æ£€ç´¢
- å…³é”®è¯æƒé‡ä¼˜åŒ–
- æ£€ç´¢ç»“æœé‡æ’åº
"""
import os
import time
import httpx
from typing import List, Optional, Dict, Any
from pathlib import Path

try:
    import chromadb
    from chromadb import Documents, EmbeddingFunction, Embeddings
except ImportError:
    raise ImportError("Please install chromadb: pip install chromadb")

from src.core.config_manager import ConfigManager
from src.core.logger import logger
from src.services.retrieval_strategy import (
    RetrievalStrategy, 
    get_retrieval_strategy,
    SceneType
)


class SiliconFlowEmbedding(EmbeddingFunction):
    """
    è‡ªå®šä¹‰åµŒå…¥å‡½æ•°ï¼Œè°ƒç”¨ API ç”Ÿæˆå‘é‡
    æ”¯æŒå¤šä¾›åº”å•†é…ç½®
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åµŒå…¥å‡½æ•°"""
        self.ai_config = ConfigManager.get_ai_config()
        
        # è·å–åµŒå…¥æ¨¡å‹çš„ä¾›åº”å•†é…ç½®
        embedding_config = self.ai_config.embedding
        provider_name = getattr(embedding_config, 'provider', '') or self.ai_config.common.default_provider
        
        # ä» providers è·å–é…ç½®
        providers = getattr(self.ai_config, 'providers', {})
        if provider_name in providers:
            provider = providers[provider_name]
            self.base_url = provider.api_base
            self.api_key = provider.api_key
            self.timeout = provider.timeout
        else:
            # å…¼å®¹æ—§é…ç½®æˆ–æŠ›å‡ºé”™è¯¯
            if hasattr(self.ai_config.common, 'api_base') and self.ai_config.common.api_base:
                self.base_url = self.ai_config.common.api_base
                self.api_key = self.ai_config.common.api_key
                self.timeout = self.ai_config.common.timeout
            else:
                raise ValueError(f"æœªæ‰¾åˆ°ä¾›åº”å•†é…ç½®: {provider_name}")
        
        self.model = embedding_config.model_name
        
        logger.info(f"ğŸ§  åˆå§‹åŒ–åµŒå…¥å‡½æ•°: {self.model} (provider: {provider_name})")

    def __call__(self, input: Documents) -> Embeddings:
        """
        å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡åˆ—è¡¨
        
        Args:
            input: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        embeddings = []
        for text in input:
            try:
                emb = self._get_embedding_sync(text)
                embeddings.append(emb)
            except Exception as e:
                logger.error(f"âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
                # è¿”å›é›¶å‘é‡ä½œä¸ºé™çº§æ–¹æ¡ˆ
                embeddings.append([0.0] * self.ai_config.embedding.vector_dim)
        
        return embeddings

    def _get_embedding_sync(self, text: str) -> List[float]:
        """
        åŒæ­¥è°ƒç”¨ç¡…åŸºæµåŠ¨ Embedding API
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "input": text,
            "encoding_format": "float"
        }
        
        try:
            with httpx.Client(timeout=self.timeout) as client:
                resp = client.post(
                    f"{self.base_url}/embeddings",
                    json=payload,
                    headers=headers
                )
                resp.raise_for_status()
                
                result = resp.json()
                if 'data' in result and len(result['data']) > 0:
                    return result['data'][0]['embedding']
                else:
                    logger.error(f"âŒ API è¿”å›å¼‚å¸¸: {result}")
                    return [0.0] * self.ai_config.embedding.vector_dim
                    
        except httpx.TimeoutException:
            logger.error("âŒ å‘é‡ API è¯·æ±‚è¶…æ—¶")
            return [0.0] * self.ai_config.embedding.vector_dim
        except Exception as e:
            logger.error(f"âŒ å‘é‡ API è¯·æ±‚å¤±è´¥: {e}")
            return [0.0] * self.ai_config.embedding.vector_dim


class VectorService:
    """
    å‘é‡æ•°æ®åº“æœåŠ¡
    
    åŠŸèƒ½ï¼š
    1. å­˜å‚¨èŠå¤©è®°å½•åˆ°å‘é‡æ•°æ®åº“
    2. æ£€ç´¢ç›¸å…³å†å²è®°å¿†
    3. ç®¡ç†ç”¨æˆ·çš„é•¿æœŸè®°å¿†
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡æœåŠ¡"""
        try:
            bot_config = ConfigManager.get_bot_config()
            ai_config = ConfigManager.get_ai_config()
            
            # è·å–æ•°æ®åº“è·¯å¾„å¹¶åˆ›å»ºç›®å½•
            db_path = bot_config.storage.vector_db_path
            Path(db_path).mkdir(parents=True, exist_ok=True)
            
            # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯ï¼ˆæŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰
            self.client = chromadb.PersistentClient(path=db_path)
            
            # 1. å¯¹è¯è®°å¿†é›†åˆï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰
            self.memory_collection = self.client.get_or_create_collection(
                name="chat_memory",
                embedding_function=SiliconFlowEmbedding(),
                metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            )
            
            # 2. çŸ¥è¯†åº“é›†åˆï¼ˆæ–°å¢ï¼šåªè¯»æ¨¡å¼ï¼Œç”±ç‹¬ç«‹ç¨‹åºå†™å…¥ï¼‰
            self.kb_collection = self.client.get_or_create_collection(
                name="knowledge_base",
                embedding_function=SiliconFlowEmbedding(),
                metadata={"hnsw:space": "cosine"}
            )
            
            # ä¿å­˜é…ç½®
            self.retrieve_count = bot_config.storage.retrieve_count
            self.similarity_threshold = bot_config.storage.similarity_threshold
            self.min_memory_length = bot_config.storage.min_memory_length
            self.max_memory_per_user = bot_config.storage.max_memory_per_user
            self.enabled = bot_config.storage.enable_vector_memory
            
            logger.info(f"âœ… å‘é‡æœåŠ¡åˆå§‹åŒ–æˆåŠŸ: {db_path}")
            logger.info(f"   - å¯¹è¯è®°å¿†é›†åˆ: chat_memory")
            logger.info(f"   - çŸ¥è¯†åº“é›†åˆ: knowledge_base")
            logger.info(f"   - æ£€ç´¢æ¡æ•°: {self.retrieve_count}")
            logger.info(f"   - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False
            raise

    def add_memory(self, user_id: str, text: str, role: str) -> bool:
        """
        å­˜å…¥ä¸€æ¡è®°å¿†
        
        Args:
            user_id: ç”¨æˆ· ID
            text: è®°å¿†å†…å®¹
            role: è§’è‰²ï¼ˆ"User" æˆ– "Bot"ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸå­˜å‚¨
        """
        if not self.enabled:
            return False
        
        # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
        if len(text) < self.min_memory_length:
            logger.debug(f"â­ï¸  è®°å¿†è¿‡çŸ­ï¼Œè·³è¿‡: {text[:30]}")
            return False
        
        # æ£€æŸ¥ç”¨æˆ·çš„ç°æœ‰è®°å¿†æ•°é‡
        existing = self._get_user_memory_count(user_id)
        if existing >= self.max_memory_per_user:
            logger.warning(f"âš ï¸  ç”¨æˆ· {user_id} è®°å¿†å·²æ»¡ ({existing})")
            return False
        
        try:
            # ç”Ÿæˆå”¯ä¸€ IDï¼ˆæ—¶é—´æˆ³ + éšæœºæ•°ï¼‰
            mem_id = f"{user_id}_{int(time.time() * 1000000)}"
            
            # å­˜å…¥å¯¹è¯è®°å¿†é›†åˆ
            self.memory_collection.add(
                documents=[text],
                metadatas=[{
                    "user_id": user_id,
                    "role": role,
                    "timestamp": int(time.time())
                }],
                ids=[mem_id]
            )
            
            logger.debug(f"ğŸ’¾ è®°å¿†å·²å­˜å‚¨: [{role}] {text[:40]}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
            return False

    def add_pair_memory(self, user_id: str, query: str, reply: str) -> bool:
        """
        å­˜å…¥ Q&A å¯¹ï¼Œè¯­ä¹‰æ›´å®Œæ•´
        
        Args:
            user_id: ç”¨æˆ· ID
            query: ç”¨æˆ·æé—®
            reply: æœºå™¨äººå›å¤
            
        Returns:
            æ˜¯å¦æˆåŠŸå­˜å‚¨
        """
        if not self.enabled:
            return False
        
        try:
            # å°† Q&A ç»„åˆæˆä¸€æ¡è®°å¿†
            combined_text = f"Useré—®: {query}\nBotç­”: {reply}"
            
            # ç”Ÿæˆå”¯ä¸€ ID
            mem_id = f"{user_id}_{int(time.time() * 1000000)}"
            
            # å­˜å…¥å¯¹è¯è®°å¿†é›†åˆ
            self.memory_collection.add(
                documents=[combined_text],
                metadatas=[{
                    "user_id": user_id,
                    "role": "Pair",
                    "timestamp": int(time.time()),
                    "query": query,
                    "reply": reply
                }],
                ids=[mem_id]
            )
            
            logger.debug(f"ğŸ’¾ Q&A å¯¹å·²å­˜å‚¨: {query[:30]} -> {reply[:30]}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨ Q&A å¯¹å¤±è´¥: {e}")
            return False

    def search_memory(
        self, 
        user_id: str, 
        query_text: str, 
        k: Optional[int] = None,
        max_tokens: int = 500
    ) -> str:
        """
        æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ—¶é—´æƒé‡ + é‡è¦åº¦ + token ä¸Šé™ï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            query_text: æŸ¥è¯¢æ–‡æœ¬
            k: æ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            max_tokens: è¿”å›è®°å¿†çš„æœ€å¤§ token æ•°ï¼ˆçº¦ç­‰äºå­—ç¬¦æ•°/2ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not self.enabled:
            return ""
        
        # ===== 1. çŸ­æ–‡æœ¬/æ— æ•ˆæ–‡æœ¬è·³è¿‡ =====
        query_stripped = query_text.strip()
        if len(query_stripped) < 4:
            logger.debug(f"â­ï¸ [{user_id}] æŸ¥è¯¢è¿‡çŸ­ï¼Œè·³è¿‡è®°å¿†æ£€ç´¢: {query_stripped}")
            return ""
        
        # è¯­æ°”è¯/æ— æ„ä¹‰è¯æ£€æµ‹
        skip_patterns = {"å—¯", "å“¦", "å¥½", "å•Š", "å‘¢", "å§", "äº†", "åœ¨å—", "åœ¨ä¸", "ä½ å¥½"}
        if query_stripped in skip_patterns:
            logger.debug(f"â­ï¸ [{user_id}] æ— æ„ä¹‰æŸ¥è¯¢ï¼Œè·³è¿‡è®°å¿†æ£€ç´¢: {query_stripped}")
            return ""
        
        try:
            # ===== 2. å‘é‡æ£€ç´¢ï¼šTop-K ç²—ç­› =====
            fetch_count = (k or self.retrieve_count) + 5  # å¤šå–å‡ æ¡ç”¨äºé‡æ’
            
            results = self.memory_collection.query(
                query_texts=[query_text],
                n_results=fetch_count,
                where={"user_id": user_id}
            )
            
            documents = results.get('documents', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0]
            
            if not documents:
                logger.debug(f"ğŸ” [{user_id}] æœªæ£€ç´¢åˆ°è®°å¿†")
                return ""
            
            # ===== 3. é‡æ’åºï¼šæ—¶é—´æƒé‡ + é‡è¦åº¦ + ç›¸ä¼¼åº¦ =====
            current_time = int(time.time())
            # æ—¶é—´è¡°å‡å‚æ•°ï¼šÏ„ = 7 å¤©ï¼ˆç§’ï¼‰
            tau = 7 * 24 * 3600
            
            scored_results = []
            for doc, meta, dist in zip(documents, metadatas, distances):
                similarity = 1 - dist
                
                # åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆçŸ­æ–‡æœ¬æ—¶æé«˜é˜ˆå€¼ï¼‰
                threshold = self.similarity_threshold
                if len(query_stripped) < 8:
                    threshold = min(threshold + 0.1, 0.7)
                
                if similarity < threshold:
                    continue
                
                # æ—¶é—´æƒé‡ï¼šfreshness = exp(-age / Ï„)
                created_at = meta.get("timestamp", 0)
                age = max(current_time - created_at, 0)
                import math
                freshness = math.exp(-age / tau)
                
                # é‡è¦åº¦æƒé‡ï¼šsummary ç±»å‹è®°å¿†æƒé‡æ›´é«˜
                importance = 1.0
                if meta.get("memory_type") == "summary":
                    importance = 1.5
                elif meta.get("role") == "Pair":
                    importance = 1.2
                
                # ç»¼åˆå¾—åˆ†ï¼šsim * (1 + Î± * freshness) + Î² * importance
                alpha = 0.3
                beta = 0.1
                combined_score = similarity * (1 + alpha * freshness) + beta * importance
                
                scored_results.append({
                    "doc": doc,
                    "meta": meta,
                    "similarity": similarity,
                    "freshness": freshness,
                    "importance": importance,
                    "score": combined_score
                })
            
            if not scored_results:
                logger.debug(f"ğŸ” [{user_id}] è®°å¿†ç›¸ä¼¼åº¦è¿‡ä½")
                return ""
            
            # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
            scored_results.sort(key=lambda x: x["score"], reverse=True)
            
            # ===== 4. Token ä¸Šé™æ§åˆ¶ =====
            memory_lines = []
            total_chars = 0
            max_chars = max_tokens * 2  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 2 å­—ç¬¦
            
            for r in scored_results:
                doc = r["doc"]
                doc_chars = len(doc)
                
                if total_chars + doc_chars > max_chars:
                    break
                
                role = r["meta"].get('role', 'Unknown')
                memory_lines.append(f"- [{role}] {doc}")
                total_chars += doc_chars
            
            if not memory_lines:
                return ""
            
            result_text = "\n".join(memory_lines)
            logger.debug(f"ğŸ” [{user_id}] æ£€ç´¢åˆ° {len(memory_lines)} æ¡è®°å¿† (çº¦ {total_chars//2} tokens)")
            return result_text
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢è®°å¿†å¤±è´¥: {e}")
            return ""

    def search_knowledge(
        self, 
        query_text: str, 
        k: Optional[int] = None,
        use_advanced_strategy: bool = True,
        conversation_history: Optional[List[str]] = None,
        max_tokens: int = 400
    ) -> str:
        """
        æ£€ç´¢çŸ¥è¯†åº“ï¼ˆv2.0 å¤šå±‚æ¬¡æ£€ç´¢ä¼˜åŒ–ç‰ˆ + token ä¸Šé™ï¼‰
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            k: æ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤ 4 æ¡ï¼Œåç»­ä¼šé‡æ’åºç­›é€‰ï¼‰
            use_advanced_strategy: æ˜¯å¦ä½¿ç”¨é«˜çº§æ£€ç´¢ç­–ç•¥
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºåœºæ™¯è¯†åˆ«ï¼‰
            max_tokens: è¿”å›çŸ¥è¯†çš„æœ€å¤§ token æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„çŸ¥è¯†ç‰‡æ®µå­—ç¬¦ä¸²ï¼ˆæŒ‰ç…§æ ‡å‡†æ ¼å¼è¾“å‡ºï¼‰
        """
        if not self.enabled:
            return ""
        
        # ===== çŸ­æ–‡æœ¬è·³è¿‡ï¼ˆçŸ¥è¯†åº“é˜ˆå€¼æ¯”è®°å¿†å®½æ¾ä¸€ç‚¹ï¼‰=====
        query_stripped = query_text.strip()
        if len(query_stripped) < 3:
            logger.debug(f"â­ï¸ æŸ¥è¯¢è¿‡çŸ­ï¼Œè·³è¿‡çŸ¥è¯†åº“æ£€ç´¢: {query_stripped}")
            return ""
        
        # çº¯è¯­æ°”è¯è·³è¿‡
        skip_patterns = {"å—¯", "å“¦", "å¥½", "å•Š", "å‘¢", "å§", "äº†"}
        if query_stripped in skip_patterns:
            return ""
        
        try:
            # ä»é…ç½®è¯»å–é»˜è®¤è¿”å›æ•°é‡
            if k is None:
                try:
                    import toml
                    from pathlib import Path
                    config_path = Path(__file__).parent.parent.parent / "configs" / "retrieval_config.toml"
                    if config_path.exists():
                        retrieval_config = toml.load(str(config_path))
                        k = retrieval_config.get("retrieval", {}).get("default_result_count", 4)
                    else:
                        k = 4
                except Exception:
                    k = 4  # é»˜è®¤ 4 æ¡
            
            # è·å–æ£€ç´¢ç­–ç•¥
            strategy = get_retrieval_strategy() if use_advanced_strategy else None
            
            # åœºæ™¯è¯†åˆ«
            scene = SceneType.UNKNOWN
            if strategy:
                scene = strategy.identify_scene(query_text, conversation_history)
                logger.debug(f"ğŸ­ æ£€ç´¢åœºæ™¯: {scene.value}")
            
            # æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆå¤šå–ä¸€äº›ç”¨äºé‡æ’åºï¼‰
            fetch_count = k * 3 if use_advanced_strategy else k
            results = self.kb_collection.query(
                query_texts=[query_text],
                n_results=fetch_count
            )
            
            # è§£æç»“æœ
            documents = results.get('documents', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0]
            
            if not documents:
                logger.debug(f"ğŸ” çŸ¥è¯†åº“æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
                return ""
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            raw_results = []
            for doc, meta, dist in zip(documents, metadatas, distances):
                similarity = 1 - dist  # ChromaDB distance è½¬ similarity
                raw_results.append({
                    "content": doc,
                    "source": meta.get('source', 'Unknown'),
                    "similarity": similarity
                })
            
            # ä½¿ç”¨é«˜çº§æ£€ç´¢ç­–ç•¥è¿›è¡Œé‡æ’åº
            if strategy:
                # é‡æ’åº
                reranked = strategy.rerank_results(raw_results, query_text, scene)
                
                # é˜ˆå€¼è¿‡æ»¤
                filtered = strategy.filter_by_threshold(reranked, min_score=self.similarity_threshold)
                
                if not filtered:
                    logger.debug(f"ğŸ” çŸ¥è¯†åº“é‡æ’åºåæ— ç¬¦åˆæ¡ä»¶çš„ç»“æœ")
                    return ""
                
                # æ ¼å¼åŒ–è¾“å‡ºï¼ˆé™åˆ¶åˆ° k æ¡ï¼‰
                actual_count = min(len(filtered), k)
                result_text = strategy.format_results(filtered, max_results=k)
                logger.info(f"ğŸ” çŸ¥è¯†åº“æ£€ç´¢åˆ° {actual_count} æ¡ç›¸å…³å†…å®¹ (åœºæ™¯: {scene.value})")
                return result_text
            
            else:
                # ç®€å•æ¨¡å¼ï¼šä»…æŒ‰ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                filtered_results = [
                    r for r in raw_results 
                    if r["similarity"] >= self.similarity_threshold
                ]
                
                if not filtered_results:
                    logger.debug(f"ğŸ” çŸ¥è¯†åº“ç›¸ä¼¼åº¦è¿‡ä½")
                    return ""
                
                # æ ¼å¼åŒ–è¾“å‡ºï¼ˆç®€å•æ¨¡å¼ï¼‰
                knowledge_lines = []
                for i, r in enumerate(filtered_results[:k], 1):
                    source = r["source"]
                    title = source.rsplit('.', 1)[0] if '.' in source else source
                    
                    knowledge_lines.append(
                        f"{i}. æ ‡é¢˜ï¼š{title}\n"
                        f"   å†…å®¹ï¼š{r['content']}\n"
                        f"   ç›¸å…³æ€§ï¼š{r['similarity']:.2f}\n"
                        f"   æœç´¢ç±»å‹ï¼švector"
                    )
                
                result_text = "\n".join(knowledge_lines)
                logger.debug(f"ğŸ” çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(filtered_results)} æ¡ç›¸å…³å†…å®¹")
                return result_text
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return ""
    
    def search_knowledge_with_context(
        self,
        query_text: str,
        conversation_history: List[str],
        k: int = 3
    ) -> str:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„çŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ¨èä½¿ç”¨ï¼‰
        
        Args:
            query_text: å½“å‰æŸ¥è¯¢
            conversation_history: æœ€è¿‘çš„å¯¹è¯å†å²
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„çŸ¥è¯†ç‰‡æ®µ
        """
        return self.search_knowledge(
            query_text=query_text,
            k=k,
            use_advanced_strategy=True,
            conversation_history=conversation_history
        )

    def _get_user_memory_count(self, user_id: str) -> int:
        """è·å–ç”¨æˆ·çš„è®°å¿†æ¡æ•°"""
        try:
            results = self.memory_collection.get(
                where={"user_id": user_id}
            )
            return len(results.get('ids', []))
        except Exception as e:
            logger.warning(f"âš ï¸  è·å–ç”¨æˆ·è®°å¿†æ•°å¤±è´¥: {e}")
            return 0

    def clear_user_memory(self, user_id: str) -> bool:
        """æ¸…ç©ºæŸä¸ªç”¨æˆ·çš„æ‰€æœ‰è®°å¿†ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
        try:
            self.memory_collection.delete(
                where={"user_id": user_id}
            )
            logger.warning(f"ğŸ—‘ï¸  å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„æ‰€æœ‰è®°å¿†")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
            return False

    def get_memory_stats(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·çš„è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            results = self.memory_collection.get(
                where={"user_id": user_id}
            )
            
            memory_count = len(results.get('ids', []))
            metadatas = results.get('metadatas', [])
            
            # ç»Ÿè®¡ä¸åŒè§’è‰²çš„è®°å¿†
            role_counts = {}
            for meta in metadatas:
                role = meta.get('role', 'Unknown')
                role_counts[role] = role_counts.get(role, 0) + 1
            
            return {
                "total": memory_count,
                "by_role": role_counts,
                "last_updated": int(time.time())
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total": 0, "error": str(e)}
    
    def get_knowledge_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        try:
            # è·å–æ‰€æœ‰çŸ¥è¯†åº“æ¡ç›®
            results = self.kb_collection.get()
            
            total_count = len(results.get('ids', []))
            metadatas = results.get('metadatas', [])
            
            # ç»Ÿè®¡ä¸åŒæ¥æºçš„çŸ¥è¯†
            source_counts = {}
            for meta in metadatas:
                source = meta.get('source', 'Unknown')
                source_counts[source] = source_counts.get(source, 0) + 1
            
            return {
                "total": total_count,
                "by_source": source_counts,
                "last_checked": int(time.time())
            }
        except Exception as e:
            logger.error(f"âŒ è·å–çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total": 0, "error": str(e)}


# å…¨å±€å•ä¾‹
_vector_service: Optional[VectorService] = None


def get_vector_service() -> VectorService:
    """è·å–å…¨å±€å‘é‡æœåŠ¡å•ä¾‹"""
    global _vector_service
    if _vector_service is None:
        _vector_service = VectorService()
    return _vector_service
