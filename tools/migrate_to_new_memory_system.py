"""
è®°å¿†ç³»ç»Ÿè¿ç§»å·¥å…· - ä»æ—§ç»“æ„è¿ç§»åˆ°æ–°çš„åŒæ•°æ®åº“ç»“æ„

æ—§ç»“æ„:
- data/chroma_db/memory.db (å•ä¸€æ•°æ®åº“)
- data/chroma_db/memory.faiss (å•ä¸€ç´¢å¼•)

æ–°ç»“æ„:
- data/memory_v2/private/{user_id}/
  - private.db (ç§èŠæ•°æ®)
  - private.faiss (ç§èŠç´¢å¼•)
  - groups.db (è¯¥ç”¨æˆ·åœ¨å„ç¾¤çš„å‘è¨€)
  - groups.faiss (ç¾¤èŠç´¢å¼•)
  
- data/memory_v2/groups/{group_id}/
  - members.db (ç¾¤æˆå‘˜æ•°æ®)
  - members.faiss (ç¾¤ç´¢å¼•)
"""
import sqlite3
import shutil
import pickle
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.logger import logger


def backup_old_database():
    """å¤‡ä»½æ—§æ•°æ®åº“"""
    old_db = project_root / "data/chroma_db/memory.db"
    old_faiss = project_root / "data/chroma_db/memory.faiss"
    old_id_map = project_root / "data/chroma_db/memory_id_map.pkl"
    
    if not old_db.exists():
        logger.error("âŒ æ—§æ•°æ®åº“ä¸å­˜åœ¨")
        return False
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = project_root / f"data/backup_{timestamp}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸ“¦ å¤‡ä»½æ—§æ•°æ®åˆ°: {backup_dir}")
    
    if old_db.exists():
        shutil.copy2(old_db, backup_dir / "memory.db")
        logger.info(f"  âœ“ memory.db")
    
    if old_faiss.exists():
        shutil.copy2(old_faiss, backup_dir / "memory.faiss")
        logger.info(f"  âœ“ memory.faiss")
    
    if old_id_map.exists():
        shutil.copy2(old_id_map, backup_dir / "memory_id_map.pkl")
        logger.info(f"  âœ“ memory_id_map.pkl")
    
    return True


def analyze_old_database():
    """åˆ†ææ—§æ•°æ®åº“"""
    old_db = project_root / "data/chroma_db/memory.db"
    
    if not old_db.exists():
        logger.error(f"âŒ æ—§æ•°æ®åº“ä¸å­˜åœ¨: {old_db}")
        raise FileNotFoundError(f"æ•°æ®åº“ä¸å­˜åœ¨: {old_db}")
    
    conn = sqlite3.connect(str(old_db))
    cursor = conn.cursor()
    
    # æ€»è®°å½•æ•°
    cursor.execute('SELECT COUNT(*) FROM memories')
    total = cursor.fetchone()[0]
    
    # ç”¨æˆ·æ•°
    cursor.execute('SELECT COUNT(DISTINCT user_id) FROM memories')
    user_count = cursor.fetchone()[0]
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    cursor.execute('SELECT role, COUNT(*) FROM memories GROUP BY role')
    by_role = {row[0]: row[1] for row in cursor.fetchall()}
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¾¤èŠæ•°æ®ï¼ˆé€šè¿‡æ£€æµ‹æ˜¯å¦æœ‰ group_id ç›¸å…³å­—æ®µæˆ–æ•°æ®ï¼‰
    cursor.execute('PRAGMA table_info(memories)')
    columns = [row[1] for row in cursor.fetchall()]
    has_group_field = 'group_id' in columns or 'scene_id' in columns
    
    conn.close()
    
    return {
        'total': total,
        'user_count': user_count,
        'by_role': by_role,
        'has_group_field': has_group_field
    }


def create_private_db(user_dir: Path):
    """åˆ›å»ºç”¨æˆ·ç§èŠæ•°æ®åº“"""
    db_path = user_dir / "private.db"
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    # ç§èŠè®°å¿†è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS private_memories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            query TEXT,
            reply TEXT,
            timestamp INTEGER NOT NULL
        )
    """)
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON private_memories(timestamp)")
    
    # ç¾¤èŠè®°å¿†è¡¨ï¼ˆè¯¥ç”¨æˆ·åœ¨å„ä¸ªç¾¤çš„å‘è¨€ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS group_memories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            group_id TEXT NOT NULL,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            query TEXT,
            reply TEXT,
            timestamp INTEGER NOT NULL
        )
    """)
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_group_timestamp ON group_memories(group_id, timestamp)")
    
    conn.commit()
    conn.close()


def create_user_groups_db(user_dir: Path):
    """åˆ›å»ºç”¨æˆ·ç¾¤èŠæ•°æ®åº“ï¼ˆå·²åºŸå¼ƒï¼Œç¾¤èŠè®°å¿†ç°åœ¨å­˜åœ¨ private.db çš„ group_memories è¡¨ä¸­ï¼‰"""
    # è¿™ä¸ªå‡½æ•°ä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼Œä½†ä¸å†åˆ›å»ºç‹¬ç«‹çš„ groups.db
    pass


def create_group_members_db(group_dir: Path):
    """åˆ›å»ºç¾¤æˆå‘˜æ•°æ®åº“"""
    db_path = group_dir / "members.db"
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS member_memories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id TEXT NOT NULL,
            sender_name TEXT,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            query TEXT,
            reply TEXT,
            timestamp INTEGER NOT NULL
        )
    """)
    
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_user_timestamp ON member_memories(user_id, timestamp)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON member_memories(timestamp)")
    conn.commit()
    conn.close()


def migrate_data():
    """è¿ç§»æ•°æ®"""
    old_db = project_root / "data/chroma_db/memory.db"
    new_base = project_root / "data/memory_v2"
    
    logger.info("\nğŸ”„ å¼€å§‹è¿ç§»æ•°æ®...")
    
    # è¿æ¥æ—§æ•°æ®åº“
    old_conn = sqlite3.connect(str(old_db))
    old_cursor = old_conn.cursor()
    
    # è·å–æ‰€æœ‰è®°å½•
    old_cursor.execute("""
        SELECT id, user_id, role, content, timestamp, query, reply
        FROM memories
        ORDER BY user_id, timestamp
    """)
    
    records = old_cursor.fetchall()
    logger.info(f"  æ€»è®°å½•æ•°: {len(records)}")
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_records = defaultdict(list)
    for record in records:
        record_id, user_id, role, content, timestamp, query, reply = record
        user_records[user_id].append({
            'id': record_id,
            'role': role,
            'content': content,
            'timestamp': timestamp,
            'query': query,
            'reply': reply
        })
    
    logger.info(f"  ç”¨æˆ·æ•°: {len(user_records)}")
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç§èŠæ•°æ®åº“
    migrated_users = 0
    migrated_records = 0
    
    for user_id, user_data in user_records.items():
        user_dir = new_base / "private" / str(user_id)
        user_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºç§èŠæ•°æ®åº“
        create_private_db(user_dir)
        
        # æ’å…¥æ•°æ®ï¼ˆç›®å‰æ‰€æœ‰æ•°æ®éƒ½å½“ä½œç§èŠï¼‰
        private_db = user_dir / "private.db"
        conn = sqlite3.connect(str(private_db))
        cursor = conn.cursor()
        
        for record in user_data:
            cursor.execute("""
                INSERT INTO private_memories (role, content, query, reply, timestamp)
                VALUES (?, ?, ?, ?, ?)
            """, (
                record['role'],
                record['content'],
                record['query'],
                record['reply'],
                record['timestamp']
            ))
            migrated_records += 1
        
        conn.commit()
        conn.close()
        
        migrated_users += 1
        
        if migrated_users % 10 == 0:
            logger.info(f"  è¿›åº¦: {migrated_users}/{len(user_records)} ç”¨æˆ·")
    
    old_conn.close()
    
    logger.info(f"\nâœ… æ•°æ®è¿ç§»å®Œæˆ:")
    logger.info(f"  è¿ç§»ç”¨æˆ·: {migrated_users}")
    logger.info(f"  è¿ç§»è®°å½•: {migrated_records}")
    
    return migrated_users, migrated_records


def rebuild_faiss_indices():
    """é‡å»º FAISS ç´¢å¼•"""
    logger.info("\nğŸ”¨ å¼€å§‹é‡å»º FAISS ç´¢å¼•...")
    logger.info("âš ï¸ è¿™éœ€è¦è°ƒç”¨ embedding APIï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´...")
    
    try:
        import faiss
        import numpy as np
    except ImportError:
        logger.error("âŒ éœ€è¦å®‰è£… faiss-cpu: pip install faiss-cpu")
        return False
    
    # åŠ è½½é…ç½®
    from src.core.config_manager import ConfigManager
    ConfigManager.load()
    
    from src.services.vector_service import EmbeddingClient
    embedding_client = EmbeddingClient()
    
    new_base = project_root / "data/memory_v2"
    private_dir = new_base / "private"
    
    if not private_dir.exists():
        logger.error("âŒ ç§èŠæ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·é‡å»ºç´¢å¼•
    user_dirs = [d for d in private_dir.iterdir() if d.is_dir()]
    logger.info(f"  æ‰¾åˆ° {len(user_dirs)} ä¸ªç”¨æˆ·ç›®å½•")
    
    success_count = 0
    error_count = 0
    
    for i, user_dir in enumerate(user_dirs, 1):
        user_id = user_dir.name
        private_db = user_dir / "private.db"
        
        if not private_db.exists():
            logger.warning(f"  âš ï¸ ç”¨æˆ· {user_id} çš„æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        try:
            # è¯»å–ç§èŠæ•°æ®
            conn = sqlite3.connect(str(private_db))
            cursor = conn.cursor()
            cursor.execute("SELECT id, content FROM private_memories ORDER BY id")
            records = cursor.fetchall()
            conn.close()
            
            if not records:
                logger.debug(f"  ç”¨æˆ· {user_id}: æ— æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # åˆ›å»º FAISS ç´¢å¼•
            vector_dim = embedding_client.vector_dim
            index = faiss.IndexFlatIP(vector_dim)
            id_map = []
            
            for record_id, content in records:
                try:
                    # ç”Ÿæˆå‘é‡
                    embedding = embedding_client.get_embedding(content)
                    
                    # å½’ä¸€åŒ–
                    norm = np.linalg.norm(embedding)
                    if norm > 0:
                        embedding = embedding / norm
                    
                    # æ·»åŠ åˆ°ç´¢å¼•
                    index.add(embedding.reshape(1, -1))
                    id_map.append(record_id)
                
                except Exception as e:
                    logger.error(f"  å¤„ç†è®°å½• {record_id} å¤±è´¥: {e}")
                    error_count += 1
            
            # ä¿å­˜ç´¢å¼•
            faiss_path = user_dir / "private.faiss"
            id_map_path = user_dir / "private_id_map.pkl"
            
            faiss.write_index(index, str(faiss_path))
            with open(id_map_path, 'wb') as f:
                pickle.dump(id_map, f)
            
            success_count += 1
            
            if i % 10 == 0:
                logger.info(f"  è¿›åº¦: {i}/{len(user_dirs)} ç”¨æˆ·")
        
        except Exception as e:
            logger.error(f"  ç”¨æˆ· {user_id} ç´¢å¼•é‡å»ºå¤±è´¥: {e}")
            error_count += 1
    
    logger.info(f"\nâœ… FAISS ç´¢å¼•é‡å»ºå®Œæˆ:")
    logger.info(f"  æˆåŠŸ: {success_count} ä¸ªç”¨æˆ·")
    logger.info(f"  å¤±è´¥: {error_count} ä¸ª")
    
    return True


def verify_migration():
    """éªŒè¯è¿ç§»ç»“æœ"""
    logger.info("\nğŸ” éªŒè¯è¿ç§»ç»“æœ...")
    
    new_base = project_root / "data/memory_v2"
    private_dir = new_base / "private"
    
    if not private_dir.exists():
        logger.error("âŒ æ–°æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # ç»Ÿè®¡æ–°æ•°æ®åº“
    user_dirs = [d for d in private_dir.iterdir() if d.is_dir()]
    total_records = 0
    total_users = len(user_dirs)
    users_with_index = 0
    
    for user_dir in user_dirs:
        private_db = user_dir / "private.db"
        private_faiss = user_dir / "private.faiss"
        
        if private_db.exists():
            conn = sqlite3.connect(str(private_db))
            cursor = conn.cursor()
            try:
                cursor.execute("SELECT COUNT(*) FROM private_memories")
                count = cursor.fetchone()[0]
                total_records += count
            except sqlite3.OperationalError:
                # è¡¨å¯èƒ½è¿˜ä¸å­˜åœ¨
                pass
            conn.close()
        
        if private_faiss.exists():
            users_with_index += 1
    
    logger.info(f"  æ–°æ•°æ®åº“ç»Ÿè®¡:")
    logger.info(f"    ç”¨æˆ·æ•°: {total_users}")
    logger.info(f"    æ€»è®°å½•æ•°: {total_records}")
    logger.info(f"    æœ‰ç´¢å¼•çš„ç”¨æˆ·: {users_with_index}/{total_users}")
    
    # å¯¹æ¯”æ—§æ•°æ®åº“
    old_db = project_root / "data/chroma_db/memory.db"
    if old_db.exists():
        conn = sqlite3.connect(str(old_db))
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM memories")
        old_total = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(DISTINCT user_id) FROM memories")
        old_users = cursor.fetchone()[0]
        conn.close()
        
        logger.info(f"\n  å¯¹æ¯”æ—§æ•°æ®åº“:")
        logger.info(f"    æ—§ç”¨æˆ·æ•°: {old_users} â†’ æ–°ç”¨æˆ·æ•°: {total_users}")
        logger.info(f"    æ—§è®°å½•æ•°: {old_total} â†’ æ–°è®°å½•æ•°: {total_records}")
        
        if total_users == old_users and total_records == old_total:
            logger.info(f"  âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            return True
        else:
            logger.warning(f"  âš ï¸ æ•°æ®æ•°é‡ä¸åŒ¹é…")
            return False
    
    return True


def update_config():
    """æ›´æ–°é…ç½®æ–‡ä»¶"""
    logger.info("\nğŸ“ æ›´æ–°é…ç½®...")
    
    config_path = project_root / "configs/bot_config.toml"
    if not config_path.exists():
        logger.warning("  âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return
    
    # è¯»å–é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if 'memory_v2' in content:
        logger.info("  é…ç½®å·²ç»æ˜¯æ–°ç‰ˆæœ¬ï¼Œè·³è¿‡")
        return
    
    # å¤‡ä»½é…ç½®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = config_path.parent / f"bot_config_backup_{timestamp}.toml"
    shutil.copy2(config_path, backup_path)
    logger.info(f"  é…ç½®å·²å¤‡ä»½: {backup_path}")
    
    # æ›´æ–°è·¯å¾„
    content = content.replace(
        'vector_db_path = "./data/chroma_db"',
        'vector_db_path = "./data/memory_v2"'
    )
    
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info("  âœ… é…ç½®å·²æ›´æ–°")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("è®°å¿†ç³»ç»Ÿè¿ç§»å·¥å…· - è¿ç§»åˆ°æ–°çš„åŒæ•°æ®åº“ç»“æ„")
    print("=" * 70)
    
    # 1. åˆ†ææ—§æ•°æ®åº“
    print("\n[1/7] åˆ†ææ—§æ•°æ®åº“...")
    stats = analyze_old_database()
    print(f"  æ€»è®°å½•æ•°: {stats['total']}")
    print(f"  ç”¨æˆ·æ•°: {stats['user_count']}")
    print(f"  è®°å¿†ç±»å‹: {stats['by_role']}")
    
    # 2. ç¡®è®¤æ“ä½œ
    print("\n[2/7] è¿ç§»è®¡åˆ’:")
    print("  âœ“ å¤‡ä»½æ—§æ•°æ®åº“")
    print("  âœ“ åˆ›å»ºæ–°çš„åŒæ•°æ®åº“ç»“æ„")
    print("  âœ“ è¿ç§»æ‰€æœ‰è®°å¿†æ•°æ®")
    print("  âœ“ é‡å»º FAISS ç´¢å¼•ï¼ˆéœ€è¦è°ƒç”¨ APIï¼‰")
    print("  âœ“ éªŒè¯æ•°æ®å®Œæ•´æ€§")
    print("  âœ“ æ›´æ–°é…ç½®æ–‡ä»¶")
    
    print("\nâš ï¸ æ³¨æ„:")
    print("  - é‡å»ºç´¢å¼•éœ€è¦è°ƒç”¨ embedding APIï¼Œå¯èƒ½éœ€è¦ 10-30 åˆ†é’Ÿ")
    print("  - æ—§æ•°æ®åº“ä¼šè¢«å¤‡ä»½ï¼Œä¸ä¼šåˆ é™¤")
    print("  - å¯ä»¥éšæ—¶å›æ»šåˆ°æ—§ç‰ˆæœ¬")
    
    confirm = input("\næ˜¯å¦ç»§ç»­? (yes/no): ").strip().lower()
    if confirm not in ['yes', 'y']:
        print("æ“ä½œå·²å–æ¶ˆ")
        return
    
    # 3. å¤‡ä»½
    print("\n[3/7] å¤‡ä»½æ—§æ•°æ®...")
    if not backup_old_database():
        print("âŒ å¤‡ä»½å¤±è´¥")
        return
    
    # 4. è¿ç§»æ•°æ®
    print("\n[4/7] è¿ç§»æ•°æ®...")
    try:
        users, records = migrate_data()
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}", exc_info=True)
        return
    
    # 5. é‡å»ºç´¢å¼•
    print("\n[5/7] é‡å»º FAISS ç´¢å¼•...")
    rebuild = input("æ˜¯å¦é‡å»ºç´¢å¼•? (yes/no, å»ºè®®é€‰ yes): ").strip().lower()
    if rebuild in ['yes', 'y']:
        try:
            rebuild_faiss_indices()
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•é‡å»ºå¤±è´¥: {e}", exc_info=True)
            print("âš ï¸ å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œ: python tools/rebuild_faiss_indices.py")
    else:
        print("  è·³è¿‡ç´¢å¼•é‡å»ºï¼ˆå¯ç¨åæ‰‹åŠ¨æ‰§è¡Œï¼‰")
    
    # 6. éªŒè¯
    print("\n[6/7] éªŒè¯è¿ç§»...")
    verify_migration()
    
    # 7. æ›´æ–°é…ç½®
    print("\n[7/7] æ›´æ–°é…ç½®...")
    update_config()
    
    # å®Œæˆ
    print("\n" + "=" * 70)
    print("âœ… è¿ç§»å®Œæˆï¼")
    print("=" * 70)
    
    print("\nğŸ“‹ è¿ç§»æ‘˜è¦:")
    print(f"  è¿ç§»ç”¨æˆ·: {users}")
    print(f"  è¿ç§»è®°å½•: {records}")
    print(f"  æ–°æ•°æ®ä½ç½®: data/memory_v2/")
    print(f"  æ—§æ•°æ®å¤‡ä»½: data/backup_*/")
    
    print("\nğŸ”§ ä¸‹ä¸€æ­¥:")
    print("  1. é‡å¯ Bot: python bot.py")
    print("  2. æµ‹è¯•ç§èŠå’Œç¾¤èŠåŠŸèƒ½")
    print("  3. å¦‚æœ‰é—®é¢˜ï¼Œå¯ä»å¤‡ä»½æ¢å¤")
    
    print("\nğŸ’¡ æç¤º:")
    print("  - æ–°ç³»ç»Ÿæ”¯æŒè·¨ç¾¤ç»„æ£€ç´¢")
    print("  - ç§èŠå’Œç¾¤èŠè®°å¿†å®Œå…¨éš”ç¦»")
    print("  - æ¯ä¸ªç”¨æˆ·/ç¾¤éƒ½æœ‰ç‹¬ç«‹çš„ç´¢å¼•")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næ“ä½œå·²ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
