"""
ChromaDB è¿ç§»åˆ° FAISS + SQLite
å°†ç°æœ‰çš„ ChromaDB æ•°æ®è¿ç§»åˆ°æ–°çš„ FAISS æ¶æ„
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

import sqlite3
import numpy as np
import faiss

try:
    import chromadb
    HAS_CHROMADB = True
except ImportError:
    HAS_CHROMADB = False
    print("âš ï¸  æœªå®‰è£… chromadb")
    print("   å¦‚éœ€è¿ç§»æ—§æ•°æ®ï¼Œè¯·å…ˆå®‰è£…: pip install chromadb")
    print("   æˆ–è€…ç›´æ¥ä½¿ç”¨æ–°ç³»ç»Ÿï¼ˆæ—§æ•°æ®ä¼šä¿ç•™ä½†ä¸ä¼šè¢«ä½¿ç”¨ï¼‰")
    response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(Y/N): ").strip().upper()
    if response != 'Y':
        sys.exit(0)

from src.core.config_manager import ConfigManager
from src.services.vector_service import EmbeddingClient


def migrate():
    """æ‰§è¡Œè¿ç§»"""
    if not HAS_CHROMADB:
        print("\nâŒ æ— æ³•æ‰§è¡Œè¿ç§»ï¼šæœªå®‰è£… chromadb")
        print("   è¯·å…ˆå®‰è£…: pip install chromadb")
        return
    
    print("=" * 60)
    print("ChromaDB â†’ FAISS + SQLite æ•°æ®è¿ç§»å·¥å…·")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    ConfigManager.load()
    bot_config = ConfigManager.get_bot_config()
    ai_config = ConfigManager.get_ai_config()
    
    db_path = Path(bot_config.storage.vector_db_path)
    vector_dim = ai_config.embedding.vector_dim
    
    # æ£€æŸ¥æ—§æ•°æ®åº“
    chroma_path = db_path / "chroma.sqlite3"
    if not chroma_path.exists():
        print("âœ… æœªå‘ç° ChromaDB æ•°æ®ï¼Œæ— éœ€è¿ç§»")
        return
    
    print(f"\nğŸ“‚ å‘ç° ChromaDB æ•°æ®: {chroma_path}")
    print("å¼€å§‹è¿ç§»...")
    
    try:
        # è¿æ¥ ChromaDB
        client = chromadb.PersistentClient(path=str(db_path))
        
        # è¿ç§»è®°å¿†é›†åˆ
        print("\n1ï¸âƒ£ è¿ç§»å¯¹è¯è®°å¿†...")
        try:
            memory_collection = client.get_collection("chat_memory")
            migrate_memory_collection(memory_collection, db_path, vector_dim)
        except Exception as e:
            print(f"   âš ï¸ è®°å¿†é›†åˆè¿ç§»å¤±è´¥: {e}")
        
        # è¿ç§»çŸ¥è¯†åº“é›†åˆ
        print("\n2ï¸âƒ£ è¿ç§»çŸ¥è¯†åº“...")
        try:
            kb_collection = client.get_collection("knowledge_base")
            migrate_kb_collection(kb_collection, db_path, vector_dim)
        except Exception as e:
            print(f"   âš ï¸ çŸ¥è¯†åº“è¿ç§»å¤±è´¥: {e}")
        
        print("\n" + "=" * 60)
        print("âœ… è¿ç§»å®Œæˆï¼")
        print("=" * 60)
        print("\næç¤ºï¼š")
        print("1. æ—§æ•°æ®å·²ä¿ç•™åœ¨åŸä½ç½®")
        print("2. æ–°æ•°æ®ä½äº:")
        print(f"   - {db_path / 'memory.db'}")
        print(f"   - {db_path / 'memory.faiss'}")
        print(f"   - {db_path / 'knowledge.db'}")
        print(f"   - {db_path / 'knowledge.faiss'}")
        print("3. ç¡®è®¤æ— è¯¯åå¯åˆ é™¤æ—§çš„ ChromaDB æ•°æ®")
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def migrate_memory_collection(collection, db_path: Path, vector_dim: int):
    """è¿ç§»è®°å¿†é›†åˆ"""
    import pickle
    
    # è·å–æ‰€æœ‰æ•°æ®
    results = collection.get(include=["embeddings", "metadatas", "documents"])
    
    ids = results.get('ids', [])
    embeddings = results.get('embeddings', [])
    metadatas = results.get('metadatas', [])
    documents = results.get('documents', [])
    
    if not ids:
        print("   â­ï¸ è®°å¿†é›†åˆä¸ºç©ºï¼Œè·³è¿‡")
        return
    
    print(f"   ğŸ“Š æ‰¾åˆ° {len(ids)} æ¡è®°å¿†")
    
    # åˆ›å»º SQLite æ•°æ®åº“
    memory_db_path = db_path / "memory.db"
    conn = sqlite3.connect(str(memory_db_path))
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS memories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id TEXT NOT NULL,
            role TEXT NOT NULL,
            content TEXT NOT NULL,
            timestamp INTEGER NOT NULL,
            query TEXT,
            reply TEXT,
            memory_type TEXT
        )
    """)
    
    # åˆ›å»º FAISS ç´¢å¼•å’Œ ID æ˜ å°„
    index = faiss.IndexFlatIP(vector_dim)
    id_map = []  # FAISS index -> SQLite id
    
    # è¿ç§»æ•°æ®
    for i, (doc, meta, emb) in enumerate(zip(documents, metadatas, embeddings)):
        user_id = meta.get('user_id', 'unknown')
        role = meta.get('role', 'Unknown')
        timestamp = meta.get('timestamp', 0)
        query = meta.get('query')
        reply = meta.get('reply')
        memory_type = meta.get('memory_type')
        
        # æ’å…¥ SQLite
        cursor.execute("""
            INSERT INTO memories (user_id, role, content, timestamp, query, reply, memory_type)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (user_id, role, doc, timestamp, query, reply, memory_type))
        
        memory_id = cursor.lastrowid
        
        # æ·»åŠ åˆ° FAISS
        vec = np.array(emb, dtype=np.float32).reshape(1, -1)
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec = vec / norm
        index.add(vec)
        
        # è®°å½• ID æ˜ å°„
        id_map.append(memory_id)
        
        if (i + 1) % 100 == 0:
            print(f"   è¿›åº¦: {i + 1}/{len(ids)}")
    
    conn.commit()
    conn.close()
    
    # ä¿å­˜ FAISS ç´¢å¼•
    memory_index_path = db_path / "memory.faiss"
    faiss.write_index(index, str(memory_index_path))
    
    # ä¿å­˜ ID æ˜ å°„
    id_map_path = db_path / "memory_id_map.pkl"
    with open(id_map_path, 'wb') as f:
        pickle.dump(id_map, f)
    
    print(f"   âœ… è®°å¿†è¿ç§»å®Œæˆ: {len(ids)} æ¡")


def migrate_kb_collection(collection, db_path: Path, vector_dim: int):
    """è¿ç§»çŸ¥è¯†åº“é›†åˆ"""
    import pickle
    
    # è·å–æ‰€æœ‰æ•°æ®
    results = collection.get(include=["embeddings", "metadatas", "documents"])
    
    ids = results.get('ids', [])
    embeddings = results.get('embeddings', [])
    metadatas = results.get('metadatas', [])
    documents = results.get('documents', [])
    
    if not ids:
        print("   â­ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè·³è¿‡")
        return
    
    print(f"   ğŸ“Š æ‰¾åˆ° {len(ids)} æ¡çŸ¥è¯†")
    
    # åˆ›å»º SQLite æ•°æ®åº“
    kb_db_path = db_path / "knowledge.db"
    conn = sqlite3.connect(str(kb_db_path))
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS knowledge (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT NOT NULL,
            content TEXT NOT NULL,
            title TEXT,
            category TEXT
        )
    """)
    
    # åˆ›å»º FAISS ç´¢å¼•å’Œ ID æ˜ å°„
    index = faiss.IndexFlatIP(vector_dim)
    id_map = []  # FAISS index -> SQLite id
    
    # è¿ç§»æ•°æ®
    for i, (doc, meta, emb) in enumerate(zip(documents, metadatas, embeddings)):
        source = meta.get('source', 'Unknown')
        title = meta.get('title')
        category = meta.get('category')
        
        # æ’å…¥ SQLite
        cursor.execute("""
            INSERT INTO knowledge (source, content, title, category)
            VALUES (?, ?, ?, ?)
        """, (source, doc, title, category))
        
        kb_id = cursor.lastrowid
        
        # æ·»åŠ åˆ° FAISS
        vec = np.array(emb, dtype=np.float32).reshape(1, -1)
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec = vec / norm
        index.add(vec)
        
        # è®°å½• ID æ˜ å°„
        id_map.append(kb_id)
        
        if (i + 1) % 100 == 0:
            print(f"   è¿›åº¦: {i + 1}/{len(ids)}")
    
    conn.commit()
    conn.close()
    
    # ä¿å­˜ FAISS ç´¢å¼•
    kb_index_path = db_path / "knowledge.faiss"
    faiss.write_index(index, str(kb_index_path))
    
    # ä¿å­˜ ID æ˜ å°„
    id_map_path = db_path / "kb_id_map.pkl"
    with open(id_map_path, 'wb') as f:
        pickle.dump(id_map, f)
    
    print(f"   âœ… çŸ¥è¯†åº“è¿ç§»å®Œæˆ: {len(ids)} æ¡")


if __name__ == "__main__":
    migrate()
