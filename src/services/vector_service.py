"""
FAISS + SQLite å‘é‡æœåŠ¡ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰
- ç§èŠæ•°æ®åº“ï¼šä¸€ä¸ªç”¨æˆ·ä¸€ä¸ªæ•°æ®åº“ï¼ŒåŒ…å«ç§èŠå’Œç¾¤èŠè®°å¿†
- ç¾¤èŠæ•°æ®åº“ï¼šä¸€ä¸ªç¾¤ä¸€ä¸ªæ•°æ®åº“ï¼ŒåŒ…å«æ‰€æœ‰æˆå‘˜çš„å‘è¨€
- FAISS: é«˜æ€§èƒ½å‘é‡æ£€ç´¢ï¼Œæ”¯æŒè·¨ç¾¤ç»„æ£€ç´¢
"""
import os
import time
import sqlite3
import pickle
import httpx
import numpy as np
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
from dataclasses import dataclass

try:
    import faiss
except ImportError:
    raise ImportError("è¯·å®‰è£… faiss: pip install faiss-cpu")

from src.core.config_manager import ConfigManager
from src.core.logger import logger


@dataclass
class MemoryMetadata:
    """è®°å¿†å…ƒæ•°æ®"""
    id: int
    user_id: str
    role: str
    content: str
    timestamp: int
    query: Optional[str] = None
    reply: Optional[str] = None
    memory_type: Optional[str] = None


@dataclass
class KnowledgeMetadata:
    """çŸ¥è¯†åº“å…ƒæ•°æ®"""
    id: int
    source: str
    content: str
    title: Optional[str] = None
    category: Optional[str] = None


class EmbeddingClient:
    """åµŒå…¥å‘é‡ç”Ÿæˆå®¢æˆ·ç«¯"""
    
    def __init__(self):
        ai_config = ConfigManager.get_ai_config()
        embedding_config = ai_config.embedding
        provider_name = getattr(embedding_config, 'provider', '') or ai_config.common.default_provider
        
        providers = getattr(ai_config, 'providers', {})
        if provider_name in providers:
            provider = providers[provider_name]
            self.base_url = provider.api_base
            self.api_key = provider.api_key
            self.timeout = provider.timeout
        else:
            raise ValueError(f"æœªæ‰¾åˆ°ä¾›åº”å•†é…ç½®: {provider_name}")
        
        self.model = embedding_config.model_name
        self.vector_dim = embedding_config.vector_dim
        
        logger.info(f"ğŸ§  åµŒå…¥å®¢æˆ·ç«¯åˆå§‹åŒ–: {self.model}")
    
    def get_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "input": text,
            "encoding_format": "float"
        }
        
        try:
            with httpx.Client(timeout=self.timeout) as client:
                resp = client.post(
                    f"{self.base_url}/embeddings",
                    json=payload,
                    headers=headers
                )
                resp.raise_for_status()
                result = resp.json()
                
                if 'data' in result and len(result['data']) > 0:
                    embedding = result['data'][0]['embedding']
                    return np.array(embedding, dtype=np.float32)
                else:
                    logger.error(f"âŒ API è¿”å›å¼‚å¸¸: {result}")
                    return np.zeros(self.vector_dim, dtype=np.float32)
        
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.vector_dim, dtype=np.float32)


class FAISSVectorService:
    """FAISS + SQLite å‘é‡æœåŠ¡ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰"""
    
    def __init__(self):
        bot_config = ConfigManager.get_bot_config()
        ai_config = ConfigManager.get_ai_config()
        
        # é…ç½®å‚æ•°
        self.db_path = Path(bot_config.storage.vector_db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        self._load_config()
        
        self.vector_dim = ai_config.embedding.vector_dim
        
        # åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()
        
        # åŒæ•°æ®åº“æ¶æ„
        self.private_db_dir = self.db_path / "private"  # ç§èŠæ•°æ®åº“ç›®å½•
        self.group_db_dir = self.db_path / "groups"     # ç¾¤èŠæ•°æ®åº“ç›®å½•
        self.private_db_dir.mkdir(parents=True, exist_ok=True)
        self.group_db_dir.mkdir(parents=True, exist_ok=True)
        
        # çŸ¥è¯†åº“æ•°æ®åº“ï¼ˆä¿æŒä¸å˜ï¼‰
        self.kb_db_path = self.db_path / "knowledge.db"
        
        # åˆå§‹åŒ–æ•°æ®åº“å’Œç´¢å¼•
        self._init_sqlite()
        self._init_faiss()
        
        # ç¼“å­˜å·²åŠ è½½çš„æ•°æ®åº“è¿æ¥å’Œç´¢å¼•
        self._private_dbs = {}  # {user_id: connection}
        self._group_dbs = {}    # {group_id: connection}
        self._private_indices = {}  # {user_id: (index, id_map)}
        self._group_indices = {}    # {group_id: (index, id_map)}
        
        # åˆå§‹åŒ–æ£€ç´¢ç»Ÿè®¡
        self._last_kb_search_stats = {}
        
        logger.info(f"âœ… FAISS å‘é‡æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰")
        logger.info(f"   - ç§èŠæ•°æ®åº“ç›®å½•: {self.private_db_dir}")
        logger.info(f"   - ç¾¤èŠæ•°æ®åº“ç›®å½•: {self.group_db_dir}")
        logger.info(f"   - çŸ¥è¯†åº“: {self.kb_db_path}")
        logger.info(f"   - å‘é‡ç»´åº¦: {self.vector_dim}")
    
    def _load_config(self):
        """åŠ è½½é…ç½®å‚æ•°"""
        bot_config = ConfigManager.get_bot_config()
        
        self.retrieve_count = bot_config.storage.retrieve_count
        self.similarity_threshold = bot_config.storage.similarity_threshold
        self.min_memory_length = bot_config.storage.min_memory_length
        self.max_memory_per_user = bot_config.storage.max_memory_per_user
        self.enabled = bot_config.storage.enable_vector_memory
        
        logger.debug(f"é…ç½®å·²åŠ è½½: é˜ˆå€¼={self.similarity_threshold}, æ£€ç´¢æ•°={self.retrieve_count}")
    
    def reload_config(self):
        """çƒ­é‡è½½é…ç½®ï¼ˆä¸é‡å¯æœåŠ¡ï¼‰"""
        old_threshold = self.similarity_threshold
        self._load_config()
        logger.info(f"ğŸ”„ é…ç½®å·²é‡è½½: é˜ˆå€¼ {old_threshold} â†’ {self.similarity_threshold}")
    
    def _init_sqlite(self):
        """åˆå§‹åŒ– SQLite æ•°æ®åº“ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰"""
        # çŸ¥è¯†åº“æ•°æ®åº“ï¼ˆä¿æŒä¸å˜ï¼‰
        conn = sqlite3.connect(str(self.kb_db_path))
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT NOT NULL,
                content TEXT NOT NULL,
                title TEXT,
                category TEXT
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_source ON knowledge(source)")
        conn.commit()
        conn.close()
        
        logger.info("âœ… çŸ¥è¯†åº“æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        logger.info("   ç§èŠå’Œç¾¤èŠæ•°æ®åº“å°†æŒ‰éœ€åˆ›å»º")
    
    def _get_private_db_path(self, user_id: str) -> Path:
        """è·å–ç”¨æˆ·ç§èŠæ•°æ®åº“è·¯å¾„"""
        return self.private_db_dir / f"user_{user_id}.db"
    
    def _get_group_db_path(self, group_id: str) -> Path:
        """è·å–ç¾¤èŠæ•°æ®åº“è·¯å¾„"""
        return self.group_db_dir / f"group_{group_id}.db"
    
    def _init_private_db(self, user_id: str):
        """åˆå§‹åŒ–ç”¨æˆ·ç§èŠæ•°æ®åº“ï¼ˆä¸€ä¸ªç”¨æˆ·ä¸€ä¸ªæ•°æ®åº“ï¼‰"""
        db_path = self._get_private_db_path(user_id)
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # ç§èŠæ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS private_memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                query TEXT,
                reply TEXT
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON private_memories(timestamp)")
        
        # ç¾¤èŠæ•°æ®è¡¨ï¼ˆè¯¥ç”¨æˆ·åœ¨å„ä¸ªç¾¤çš„å‘è¨€ï¼‰
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS group_memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                query TEXT,
                reply TEXT
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_group_timestamp ON group_memories(group_id, timestamp)")
        
        conn.commit()
        conn.close()
        logger.debug(f"âœ… åˆå§‹åŒ–ç”¨æˆ· {user_id} çš„ç§èŠæ•°æ®åº“")
    
    def _init_group_db(self, group_id: str):
        """åˆå§‹åŒ–ç¾¤èŠæ•°æ®åº“ï¼ˆä¸€ä¸ªç¾¤ä¸€ä¸ªæ•°æ®åº“ï¼‰"""
        db_path = self._get_group_db_path(group_id)
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # ç¾¤æˆå‘˜è®°å¿†è¡¨ï¼ˆæ¯ä¸ªç”¨æˆ·çš„å‘è¨€ï¼‰
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS member_memories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp INTEGER NOT NULL,
                sender_name TEXT,
                query TEXT,
                reply TEXT
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_user_timestamp ON member_memories(user_id, timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON member_memories(timestamp)")
        
        conn.commit()
        conn.close()
        logger.debug(f"âœ… åˆå§‹åŒ–ç¾¤ {group_id} çš„ç¾¤èŠæ•°æ®åº“")
    
    def _init_faiss(self):
        """åˆå§‹åŒ– FAISS ç´¢å¼•ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰"""
        # çŸ¥è¯†åº“ç´¢å¼•ï¼ˆä¿æŒä¸å˜ï¼‰
        kb_index_path = self.db_path / "knowledge.faiss"
        kb_id_map_path = self.db_path / "kb_id_map.pkl"
        
        if kb_index_path.exists():
            self.kb_index = faiss.read_index(str(kb_index_path))
            if kb_id_map_path.exists():
                with open(kb_id_map_path, 'rb') as f:
                    self.kb_id_map = pickle.load(f)
            else:
                self.kb_id_map = []
            logger.info(f"   - åŠ è½½çŸ¥è¯†åº“ç´¢å¼•: {self.kb_index.ntotal} æ¡")
        else:
            self.kb_index = faiss.IndexFlatIP(self.vector_dim)
            self.kb_id_map = []
            logger.info(f"   - åˆ›å»ºæ–°çŸ¥è¯†åº“ç´¢å¼•")
        
        logger.info("   - ç§èŠå’Œç¾¤èŠç´¢å¼•å°†æŒ‰éœ€åŠ è½½")
    
    def _get_private_index_path(self, user_id: str) -> Tuple[Path, Path]:
        """è·å–ç”¨æˆ·ç§èŠç´¢å¼•è·¯å¾„"""
        index_path = self.private_db_dir / f"user_{user_id}.faiss"
        id_map_path = self.private_db_dir / f"user_{user_id}_id_map.pkl"
        return index_path, id_map_path
    
    def _get_group_index_path(self, group_id: str) -> Tuple[Path, Path]:
        """è·å–ç¾¤èŠç´¢å¼•è·¯å¾„"""
        index_path = self.group_db_dir / f"group_{group_id}.faiss"
        id_map_path = self.group_db_dir / f"group_{group_id}_id_map.pkl"
        return index_path, id_map_path
    
    def _load_private_index(self, user_id: str) -> Tuple:
        """åŠ è½½ç”¨æˆ·ç§èŠç´¢å¼•"""
        if user_id in self._private_indices:
            return self._private_indices[user_id]
        
        index_path, id_map_path = self._get_private_index_path(user_id)
        
        if index_path.exists():
            index = faiss.read_index(str(index_path))
            if id_map_path.exists():
                with open(id_map_path, 'rb') as f:
                    id_map = pickle.load(f)
            else:
                id_map = []
        else:
            index = faiss.IndexFlatIP(self.vector_dim)
            id_map = []
        
        self._private_indices[user_id] = (index, id_map)
        return index, id_map
    
    def _load_group_index(self, group_id: str) -> Tuple:
        """åŠ è½½ç¾¤èŠç´¢å¼•"""
        if group_id in self._group_indices:
            return self._group_indices[group_id]
        
        index_path, id_map_path = self._get_group_index_path(group_id)
        
        if index_path.exists():
            index = faiss.read_index(str(index_path))
            if id_map_path.exists():
                with open(id_map_path, 'rb') as f:
                    id_map = pickle.load(f)
            else:
                id_map = []
        else:
            index = faiss.IndexFlatIP(self.vector_dim)
            id_map = []
        
        self._group_indices[group_id] = (index, id_map)
        return index, id_map
    
    def _save_private_index(self, user_id: str):
        """ä¿å­˜ç”¨æˆ·ç§èŠç´¢å¼•"""
        if user_id not in self._private_indices:
            return
        
        index, id_map = self._private_indices[user_id]
        index_path, id_map_path = self._get_private_index_path(user_id)
        
        faiss.write_index(index, str(index_path))
        with open(id_map_path, 'wb') as f:
            pickle.dump(id_map, f)
    
    def _save_group_index(self, group_id: str):
        """ä¿å­˜ç¾¤èŠç´¢å¼•"""
        if group_id not in self._group_indices:
            return
        
        index, id_map = self._group_indices[group_id]
        index_path, id_map_path = self._get_group_index_path(group_id)
        
        faiss.write_index(index, str(index_path))
        with open(id_map_path, 'wb') as f:
            pickle.dump(id_map, f)
    
    def _save_faiss_index(self, index_type: str):
        """ä¿å­˜ FAISS ç´¢å¼•å’Œ ID æ˜ å°„åˆ°ç£ç›˜"""
        if index_type == "knowledge":
            kb_index_path = self.db_path / "knowledge.faiss"
            kb_id_map_path = self.db_path / "kb_id_map.pkl"
            faiss.write_index(self.kb_index, str(kb_index_path))
            with open(kb_id_map_path, 'wb') as f:
                pickle.dump(self.kb_id_map, f)
        else:
            logger.warning(f"æœªçŸ¥çš„ç´¢å¼•ç±»å‹: {index_type}ï¼Œä½¿ç”¨æ–°çš„ä¿å­˜æ–¹æ³•")
    
    def _normalize_vector(self, vec: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºå†…ç§¯ç›¸ä¼¼åº¦ï¼‰"""
        norm = np.linalg.norm(vec)
        if norm > 0:
            return vec / norm
        return vec
    
    def add_memory(self, user_id: str, text: str, role: str) -> bool:
        """
        æ·»åŠ å•æ¡è®°å¿†ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ add_pair_memoryï¼‰
        ä¿ç•™æ­¤æ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç 
        """
        logger.warning("add_memory æ–¹æ³•å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ add_pair_memory")
        return False
    
    def add_pair_memory(
        self, 
        user_id: str, 
        query: str, 
        reply: str,
        group_id: str = None,
        sender_name: str = None
    ) -> bool:
        """
        æ·»åŠ  Q&A å¯¹è®°å¿†ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            query: ç”¨æˆ·é—®é¢˜
            reply: Botå›å¤
            group_id: ç¾¤IDï¼ˆå¦‚æœæ˜¯ç¾¤èŠï¼‰
            sender_name: å‘é€è€…æ˜µç§°ï¼ˆç¾¤èŠæ—¶ä½¿ç”¨ï¼‰
        """
        if not self.enabled:
            return False
        
        combined_text = f"Useré—®: {query}\nBotç­”: {reply}"
        
        try:
            # ç”Ÿæˆå‘é‡
            embedding = self.embedding_client.get_embedding(combined_text)
            embedding = self._normalize_vector(embedding)
            
            if group_id:
                # ç¾¤èŠè®°å¿†ï¼šå­˜å‚¨åˆ°ä¸¤ä¸ªåœ°æ–¹
                # 1. ç”¨æˆ·çš„ç§èŠæ•°æ®åº“ï¼ˆgroup_memories è¡¨ï¼‰
                self._add_to_user_group_memory(user_id, group_id, query, reply, combined_text, embedding)
                
                # 2. ç¾¤çš„æ•°æ®åº“ï¼ˆmember_memories è¡¨ï¼‰
                self._add_to_group_member_memory(group_id, user_id, query, reply, combined_text, embedding, sender_name)
            else:
                # ç§èŠè®°å¿†ï¼šåªå­˜å‚¨åˆ°ç”¨æˆ·çš„ç§èŠæ•°æ®åº“ï¼ˆprivate_memories è¡¨ï¼‰
                self._add_to_user_private_memory(user_id, query, reply, combined_text, embedding)
            
            logger.debug(f"ğŸ’¾ è®°å¿†å·²å­˜å‚¨: user={user_id}, group={group_id}, query={query[:30]}")
            return True
        
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
            return False
    
    def _add_to_user_private_memory(self, user_id: str, query: str, reply: str, combined_text: str, embedding: np.ndarray):
        """æ·»åŠ åˆ°ç”¨æˆ·çš„ç§èŠè®°å¿†"""
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        db_path = self._get_private_db_path(user_id)
        if not db_path.exists():
            self._init_private_db(user_id)
        
        # å­˜å‚¨å…ƒæ•°æ®
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO private_memories (role, content, timestamp, query, reply)
            VALUES (?, ?, ?, ?, ?)
        """, ("Pair", combined_text, int(time.time()), query, reply))
        memory_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # æ·»åŠ å‘é‡åˆ° FAISS
        index, id_map = self._load_private_index(user_id)
        index.add(embedding.reshape(1, -1))
        id_map.append(memory_id)
        self._private_indices[user_id] = (index, id_map)
        self._save_private_index(user_id)
    
    def _add_to_user_group_memory(self, user_id: str, group_id: str, query: str, reply: str, combined_text: str, embedding: np.ndarray):
        """æ·»åŠ åˆ°ç”¨æˆ·çš„ç¾¤èŠè®°å¿†ï¼ˆç”¨æˆ·è§†è§’ï¼‰"""
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        db_path = self._get_private_db_path(user_id)
        if not db_path.exists():
            self._init_private_db(user_id)
        
        # å­˜å‚¨å…ƒæ•°æ®
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO group_memories (group_id, role, content, timestamp, query, reply)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (group_id, "Pair", combined_text, int(time.time()), query, reply))
        memory_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # æ·»åŠ å‘é‡åˆ°ç”¨æˆ·çš„ç§èŠç´¢å¼•ï¼ˆåŒ…å«ç¾¤èŠè®°å¿†ï¼‰
        index, id_map = self._load_private_index(user_id)
        index.add(embedding.reshape(1, -1))
        id_map.append(('group', memory_id))  # æ ‡è®°ä¸ºç¾¤èŠè®°å¿†
        self._private_indices[user_id] = (index, id_map)
        self._save_private_index(user_id)
    
    def _add_to_group_member_memory(self, group_id: str, user_id: str, query: str, reply: str, combined_text: str, embedding: np.ndarray, sender_name: str = None):
        """æ·»åŠ åˆ°ç¾¤çš„æˆå‘˜è®°å¿†ï¼ˆç¾¤è§†è§’ï¼‰"""
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        db_path = self._get_group_db_path(group_id)
        if not db_path.exists():
            self._init_group_db(group_id)
        
        # å­˜å‚¨å…ƒæ•°æ®
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO member_memories (user_id, role, content, timestamp, sender_name, query, reply)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (user_id, "Pair", combined_text, int(time.time()), sender_name, query, reply))
        memory_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # æ·»åŠ å‘é‡åˆ°ç¾¤ç´¢å¼•
        index, id_map = self._load_group_index(group_id)
        index.add(embedding.reshape(1, -1))
        id_map.append(memory_id)
        self._group_indices[group_id] = (index, id_map)
        self._save_group_index(group_id)
    
    def search_memory(
        self, 
        user_id: str, 
        query_text: str,
        group_id: str = None,
        k: Optional[int] = None,
        max_tokens: int = 500,
        cross_scene: bool = False
    ) -> str:
        """
        æ£€ç´¢è®°å¿†ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼Œæ”¯æŒè·¨ç¾¤ç»„æ£€ç´¢ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            query_text: æŸ¥è¯¢æ–‡æœ¬
            group_id: ç¾¤IDï¼ˆå¦‚æœæ˜¯ç¾¤èŠï¼‰
            k: æ£€ç´¢æ•°é‡
            max_tokens: æœ€å¤§tokenæ•°
            cross_scene: æ˜¯å¦è·¨åœºæ™¯æ£€ç´¢ï¼ˆæ£€ç´¢ç”¨æˆ·åœ¨æ‰€æœ‰ç¾¤çš„è®°å¿†ï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
        """
        if not self.enabled:
            return ""
        
        # æ¯æ¬¡æ£€ç´¢å‰é‡æ–°è¯»å–é˜ˆå€¼
        current_threshold = ConfigManager.get_bot_config().storage.similarity_threshold
        if current_threshold != self.similarity_threshold:
            logger.debug(f"æ£€æµ‹åˆ°é˜ˆå€¼å˜åŒ–: {self.similarity_threshold} â†’ {current_threshold}")
            self.similarity_threshold = current_threshold
        
        # çŸ­æ–‡æœ¬è¿‡æ»¤
        query_stripped = query_text.strip()
        if len(query_stripped) < 4:
            return ""
        
        skip_patterns = {"å—¯", "å“¦", "å¥½", "å•Š", "å‘¢", "å§", "äº†", "åœ¨å—", "åœ¨ä¸", "ä½ å¥½"}
        if query_stripped in skip_patterns:
            return ""
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vec = self.embedding_client.get_embedding(query_text)
            query_vec = self._normalize_vector(query_vec)
            
            if group_id:
                # ç¾¤èŠæ£€ç´¢ï¼šä»ç¾¤æ•°æ®åº“æ£€ç´¢
                return self._search_group_memory(group_id, user_id, query_vec, k, max_tokens, cross_scene)
            else:
                # ç§èŠæ£€ç´¢ï¼šä»ç”¨æˆ·ç§èŠæ•°æ®åº“æ£€ç´¢
                return self._search_private_memory(user_id, query_vec, k, max_tokens, cross_scene)
        
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢è®°å¿†å¤±è´¥: {e}")
            return ""
    
    def _search_private_memory(
        self,
        user_id: str,
        query_vec: np.ndarray,
        k: Optional[int],
        max_tokens: int,
        cross_scene: bool
    ) -> str:
        """æ£€ç´¢ç”¨æˆ·ç§èŠè®°å¿†"""
        db_path = self._get_private_db_path(user_id)
        if not db_path.exists():
            logger.info(f"ğŸ” [{user_id}] ç”¨æˆ·æ•°æ®åº“ä¸å­˜åœ¨")
            return ""
        
        # åŠ è½½ç´¢å¼•
        index, id_map = self._load_private_index(user_id)
        
        if index.ntotal == 0:
            logger.info(f"ğŸ” [{user_id}] ç§èŠç´¢å¼•ä¸ºç©º")
            return ""
        
        # FAISS æ£€ç´¢
        fetch_count = (k or self.retrieve_count) + 5
        distances, indices = index.search(
            query_vec.reshape(1, -1),
            min(fetch_count, index.ntotal)
        )
        
        if len(indices[0]) == 0:
            return ""
        
        # ä»æ•°æ®åº“æ‹‰å–å…ƒæ•°æ®
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        valid_results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx >= len(id_map):
                continue
            
            memory_ref = id_map[idx]
            similarity = float(dist)
            
            if similarity < self.similarity_threshold:
                continue
            
            # åˆ¤æ–­æ˜¯ç§èŠè®°å¿†è¿˜æ˜¯ç¾¤èŠè®°å¿†
            if isinstance(memory_ref, tuple):
                # ç¾¤èŠè®°å¿†ï¼š('group', memory_id)
                if not cross_scene:
                    continue  # ç§èŠæ—¶ä¸æ£€ç´¢ç¾¤èŠè®°å¿†ï¼ˆé™¤éå¼€å¯è·¨åœºæ™¯ï¼‰
                
                table_name = 'group_memories'
                memory_id = memory_ref[1]
            else:
                # ç§èŠè®°å¿†ï¼šmemory_id
                table_name = 'private_memories'
                memory_id = memory_ref
            
            # æ‹‰å–å…ƒæ•°æ®
            cursor.execute(f"""
                SELECT id, role, content, timestamp
                FROM {table_name} WHERE id = ?
            """, (memory_id,))
            
            row = cursor.fetchone()
            if row:
                valid_results.append({
                    "id": row[0],
                    "role": row[1],
                    "content": row[2],
                    "timestamp": row[3],
                    "similarity": similarity,
                    "source": table_name
                })
        
        conn.close()
        
        if not valid_results:
            logger.info(f"ğŸ” [{user_id}] æœªæ£€ç´¢åˆ°ç¬¦åˆæ¡ä»¶çš„è®°å¿†ï¼ˆé˜ˆå€¼: {self.similarity_threshold}ï¼‰")
            return ""
        
        # æ—¶é—´æƒé‡é‡æ’åº
        current_time = int(time.time())
        tau = 7 * 24 * 3600
        
        for r in valid_results:
            age = max(current_time - r["timestamp"], 0)
            import math
            freshness = math.exp(-age / tau)
            r["score"] = r["similarity"] * (1 + 0.3 * freshness)
        
        valid_results.sort(key=lambda x: x["score"], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        return self._format_memory_results(valid_results, max_tokens, user_id)
    
    def _search_group_memory(
        self,
        group_id: str,
        user_id: str,
        query_vec: np.ndarray,
        k: Optional[int],
        max_tokens: int,
        cross_scene: bool
    ) -> str:
        """æ£€ç´¢ç¾¤èŠè®°å¿†ï¼ˆæ”¯æŒè·¨ç¾¤æ£€ç´¢ï¼‰"""
        db_path = self._get_group_db_path(group_id)
        if not db_path.exists():
            logger.info(f"ğŸ” [ç¾¤{group_id}] ç¾¤æ•°æ®åº“ä¸å­˜åœ¨")
            return ""
        
        # åŠ è½½ç¾¤ç´¢å¼•
        index, id_map = self._load_group_index(group_id)
        
        if index.ntotal == 0:
            logger.info(f"ğŸ” [ç¾¤{group_id}] ç¾¤ç´¢å¼•ä¸ºç©º")
            return ""
        
        # FAISS æ£€ç´¢
        fetch_count = (k or self.retrieve_count) + 5
        distances, indices = index.search(
            query_vec.reshape(1, -1),
            min(fetch_count, index.ntotal)
        )
        
        if len(indices[0]) == 0:
            return ""
        
        # ä»æ•°æ®åº“æ‹‰å–å…ƒæ•°æ®
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        valid_results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx >= len(id_map):
                continue
            
            memory_id = id_map[idx]
            similarity = float(dist)
            
            if similarity < self.similarity_threshold:
                continue
            
            # æ‹‰å–å…ƒæ•°æ®
            cursor.execute("""
                SELECT id, user_id, role, content, timestamp, sender_name
                FROM member_memories WHERE id = ?
            """, (memory_id,))
            
            row = cursor.fetchone()
            if row:
                valid_results.append({
                    "id": row[0],
                    "user_id": row[1],
                    "role": row[2],
                    "content": row[3],
                    "timestamp": row[4],
                    "sender_name": row[5],
                    "similarity": similarity
                })
        
        conn.close()
        
        # å¦‚æœå¼€å¯è·¨åœºæ™¯æ£€ç´¢ï¼Œè¿˜è¦æ£€ç´¢è¯¥ç”¨æˆ·åœ¨å…¶ä»–ç¾¤çš„è®°å¿†
        if cross_scene:
            user_group_results = self._search_user_in_other_groups(user_id, group_id, query_vec, k)
            valid_results.extend(user_group_results)
        
        if not valid_results:
            logger.info(f"ğŸ” [ç¾¤{group_id}] æœªæ£€ç´¢åˆ°ç¬¦åˆæ¡ä»¶çš„è®°å¿†ï¼ˆé˜ˆå€¼: {self.similarity_threshold}ï¼‰")
            return ""
        
        # æ—¶é—´æƒé‡é‡æ’åº
        current_time = int(time.time())
        tau = 7 * 24 * 3600
        
        for r in valid_results:
            age = max(current_time - r["timestamp"], 0)
            import math
            freshness = math.exp(-age / tau)
            r["score"] = r["similarity"] * (1 + 0.3 * freshness)
        
        valid_results.sort(key=lambda x: x["score"], reverse=True)
        
        # æ ¼å¼åŒ–è¾“å‡º
        return self._format_memory_results(valid_results, max_tokens, f"ç¾¤{group_id}")
    
    def _search_user_in_other_groups(
        self,
        user_id: str,
        current_group_id: str,
        query_vec: np.ndarray,
        k: Optional[int]
    ) -> List[Dict]:
        """æ£€ç´¢ç”¨æˆ·åœ¨å…¶ä»–ç¾¤çš„è®°å¿†ï¼ˆè·¨ç¾¤æ£€ç´¢ï¼‰"""
        user_db_path = self._get_private_db_path(user_id)
        if not user_db_path.exists():
            return []
        
        # ä»ç”¨æˆ·æ•°æ®åº“çš„ group_memories è¡¨æ£€ç´¢
        conn = sqlite3.connect(str(user_db_path))
        cursor = conn.cursor()
        
        # è·å–ç”¨æˆ·åœ¨å…¶ä»–ç¾¤çš„è®°å¿†
        cursor.execute("""
            SELECT id, group_id, content, timestamp
            FROM group_memories
            WHERE group_id != ?
            ORDER BY timestamp DESC
            LIMIT 50
        """, (current_group_id,))
        
        other_group_memories = cursor.fetchall()
        conn.close()
        
        if not other_group_memories:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        results = []
        for memory_id, group_id, content, timestamp in other_group_memories:
            try:
                mem_vec = self.embedding_client.get_embedding(content)
                mem_vec = self._normalize_vector(mem_vec)
                similarity = float(np.dot(query_vec, mem_vec))
                
                if similarity >= self.similarity_threshold:
                    results.append({
                        "id": memory_id,
                        "user_id": user_id,
                        "role": "Pair",
                        "content": content,
                        "timestamp": timestamp,
                        "sender_name": f"[æ¥è‡ªç¾¤{group_id}]",
                        "similarity": similarity
                    })
            except Exception as e:
                logger.debug(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
                continue
        
        return results
    
    def _format_memory_results(self, results: List[Dict], max_tokens: int, context: str) -> str:
        """æ ¼å¼åŒ–è®°å¿†æ£€ç´¢ç»“æœ"""
        memory_lines = []
        total_chars = 0
        max_chars = max_tokens * 2
        
        from datetime import datetime
        
        for r in results:
            content = r["content"]
            if total_chars + len(content) > max_chars:
                break
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³
            timestamp = r["timestamp"]
            time_str = datetime.fromtimestamp(timestamp).strftime("%m-%d %H:%M")
            
            # æ·»åŠ å‘é€è€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            sender_info = ""
            if "sender_name" in r and r["sender_name"]:
                sender_info = f" {r['sender_name']}"
            
            memory_lines.append(f"- [{time_str}]{sender_info} [{r['role']}] {content}")
            total_chars += len(content)
        
        if not memory_lines:
            return ""
        
        result_text = "\n".join(memory_lines)
        logger.info(f"ğŸ” [{context}] æ£€ç´¢åˆ° {len(memory_lines)} æ¡è®°å¿†")
        return result_text
    
    def search_knowledge(
        self, 
        query_text: str, 
        k: Optional[int] = None,
        max_tokens: int = 400
    ) -> str:
        """æ£€ç´¢çŸ¥è¯†åº“"""
        if not self.enabled:
            logger.debug("çŸ¥è¯†åº“æ£€ç´¢æœªå¯ç”¨")
            self._last_kb_search_stats = {"enabled": False}
            return ""
        
        query_stripped = query_text.strip()
        if len(query_stripped) < 3:
            logger.debug(f"æŸ¥è¯¢æ–‡æœ¬è¿‡çŸ­ï¼ˆ{len(query_stripped)}å­—ï¼‰ï¼Œè·³è¿‡æ£€ç´¢")
            self._last_kb_search_stats = {"skipped": "query_too_short", "query_length": len(query_stripped)}
            return ""
        
        skip_patterns = {"å—¯", "å“¦", "å¥½", "å•Š", "å‘¢", "å§", "äº†"}
        if query_stripped in skip_patterns:
            logger.debug(f"æŸ¥è¯¢æ–‡æœ¬ '{query_stripped}' åœ¨è·³è¿‡åˆ—è¡¨ä¸­")
            self._last_kb_search_stats = {"skipped": "skip_pattern", "query": query_stripped}
            return ""
        
        logger.info(f"ğŸ“š [çŸ¥è¯†åº“æ£€ç´¢] æŸ¥è¯¢: {query_text[:50]}")
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vec = self.embedding_client.get_embedding(query_text)
            query_vec = self._normalize_vector(query_vec)
            
            # FAISS æ£€ç´¢
            fetch_count = (k or 4) * 2
            distances, indices = self.kb_index.search(
                query_vec.reshape(1, -1),
                min(fetch_count, self.kb_index.ntotal)
            )
            
            logger.info(f"   FAISS æ£€ç´¢: è¯·æ±‚ {fetch_count} æ¡ï¼Œè¿”å› {len(indices[0])} æ¡")
            
            if len(indices[0]) == 0:
                logger.info(f"   æœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
                self._last_kb_search_stats = {
                    "total_in_db": self.kb_index.ntotal,
                    "fetched": 0,
                    "passed": 0,
                    "filtered": 0
                }
                return ""
            
            # ä» SQLite æ‹‰å–å…ƒæ•°æ®
            conn = sqlite3.connect(str(self.kb_db_path))
            cursor = conn.cursor()
            
            # çŸ¥è¯†åº“é˜ˆå€¼
            kb_threshold = getattr(ConfigManager.get_bot_config().storage, 'kb_similarity_threshold', 0.45)
            logger.info(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {kb_threshold}")
            
            valid_results = []
            filtered_count = 0
            
            for idx, dist in zip(indices[0], distances[0]):
                if idx >= len(self.kb_id_map):
                    continue
                
                kb_id = self.kb_id_map[idx]
                similarity = float(dist)
                
                logger.debug(f"     çŸ¥è¯† {kb_id}: ç›¸ä¼¼åº¦ {similarity:.3f}")
                
                if similarity < kb_threshold:
                    logger.debug(f"       âœ— ç›¸ä¼¼åº¦ {similarity:.3f} < é˜ˆå€¼ {kb_threshold}ï¼Œè¿‡æ»¤")
                    filtered_count += 1
                    continue
                
                # æ‹‰å–å…ƒæ•°æ®
                cursor.execute("""
                    SELECT id, source, content, title
                    FROM knowledge WHERE id = ?
                """, (kb_id,))
                
                row = cursor.fetchone()
                if row:
                    valid_results.append({
                        "source": row[1],
                        "content": row[2],
                        "title": row[3] or row[1],
                        "similarity": similarity
                    })
                    logger.debug(f"       âœ“ çŸ¥è¯† {kb_id} é€šè¿‡: {row[3][:30]}...")
            
            conn.close()
            
            logger.info(f"   è¿‡æ»¤ç»“æœ: {len(valid_results)} æ¡é€šè¿‡ï¼Œ{filtered_count} æ¡è¢«è¿‡æ»¤")
            
            # ä¿å­˜æ£€ç´¢ç»Ÿè®¡
            self._last_kb_search_stats = {
                "total_in_db": self.kb_index.ntotal,
                "fetched": len(indices[0]),
                "passed": len(valid_results),
                "filtered": filtered_count,
                "threshold": kb_threshold
            }
            
            if not valid_results:
                logger.info(f"   æ— ç¬¦åˆæ¡ä»¶çš„çŸ¥è¯†ï¼ˆé˜ˆå€¼: {kb_threshold}ï¼‰")
                return ""
            
            # æ ¼å¼åŒ–è¾“å‡º
            knowledge_lines = []
            for i, r in enumerate(valid_results[:(k or 4)], 1):
                knowledge_lines.append(
                    f"{i}. æ ‡é¢˜ï¼š{r['title']}\n"
                    f"   å†…å®¹ï¼š{r['content']}\n"
                    f"   ç›¸å…³æ€§ï¼š{r['similarity']:.2f}"
                )
            
            result_text = "\n".join(knowledge_lines)
            logger.info(f"âœ… [çŸ¥è¯†åº“æ£€ç´¢] è¿”å› {len(knowledge_lines)} æ¡çŸ¥è¯†ï¼ˆå…± {len(result_text)} å­—ç¬¦ï¼‰")
            
            return result_text
        
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            self._last_kb_search_stats = {"error": str(e)}
            return ""
    
    def clear_user_memory(self, user_id: str) -> bool:
        """æ¸…ç©ºç”¨æˆ·è®°å¿†ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰"""
        try:
            # åˆ é™¤ç”¨æˆ·çš„ç§èŠæ•°æ®åº“
            db_path = self._get_private_db_path(user_id)
            if db_path.exists():
                db_path.unlink()
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç”¨æˆ· {user_id} çš„ç§èŠæ•°æ®åº“")
            
            # åˆ é™¤ç”¨æˆ·çš„ç§èŠç´¢å¼•
            index_path, id_map_path = self._get_private_index_path(user_id)
            if index_path.exists():
                index_path.unlink()
            if id_map_path.exists():
                id_map_path.unlink()
            
            # æ¸…é™¤ç¼“å­˜
            if user_id in self._private_indices:
                del self._private_indices[user_id]
            
            logger.warning(f"ğŸ—‘ï¸ å·²æ¸…ç©ºç”¨æˆ· {user_id} çš„æ‰€æœ‰è®°å¿†")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")
            return False
    
    def clear_group_memory(self, group_id: str) -> bool:
        """æ¸…ç©ºç¾¤èŠè®°å¿†"""
        try:
            # åˆ é™¤ç¾¤çš„æ•°æ®åº“
            db_path = self._get_group_db_path(group_id)
            if db_path.exists():
                db_path.unlink()
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¾¤ {group_id} çš„æ•°æ®åº“")
            
            # åˆ é™¤ç¾¤çš„ç´¢å¼•
            index_path, id_map_path = self._get_group_index_path(group_id)
            if index_path.exists():
                index_path.unlink()
            if id_map_path.exists():
                id_map_path.unlink()
            
            # æ¸…é™¤ç¼“å­˜
            if group_id in self._group_indices:
                del self._group_indices[group_id]
            
            logger.warning(f"ğŸ—‘ï¸ å·²æ¸…ç©ºç¾¤ {group_id} çš„æ‰€æœ‰è®°å¿†")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç¾¤è®°å¿†å¤±è´¥: {e}")
            return False
    
    def _rebuild_memory_index(self):
        """é‡å»ºè®°å¿†ç´¢å¼•ï¼ˆå·²åºŸå¼ƒï¼ŒåŒæ•°æ®åº“æ¶æ„ä¸éœ€è¦ï¼‰"""
        logger.warning("åŒæ•°æ®åº“æ¶æ„ä¸éœ€è¦é‡å»ºå…¨å±€ç´¢å¼•")
        pass
    
    def get_memory_stats(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·è®°å¿†ç»Ÿè®¡ï¼ˆåŒæ•°æ®åº“æ¶æ„ï¼‰"""
        try:
            db_path = self._get_private_db_path(user_id)
            if not db_path.exists():
                return {"total": 0, "private": 0, "group": 0}
            
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            # ç§èŠè®°å¿†æ•°
            cursor.execute("SELECT COUNT(*) FROM private_memories")
            private_count = cursor.fetchone()[0]
            
            # ç¾¤èŠè®°å¿†æ•°
            cursor.execute("SELECT COUNT(*) FROM group_memories")
            group_count = cursor.fetchone()[0]
            
            # æŒ‰ç¾¤ç»Ÿè®¡
            cursor.execute("""
                SELECT group_id, COUNT(*) 
                FROM group_memories 
                GROUP BY group_id
            """)
            by_group = {row[0]: row[1] for row in cursor.fetchall()}
            
            conn.close()
            
            return {
                "total": private_count + group_count,
                "private": private_count,
                "group": group_count,
                "by_group": by_group,
                "last_updated": int(time.time())
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total": 0, "error": str(e)}
    
    def get_group_stats(self, group_id: str) -> Dict[str, Any]:
        """è·å–ç¾¤èŠè®°å¿†ç»Ÿè®¡"""
        try:
            db_path = self._get_group_db_path(group_id)
            if not db_path.exists():
                return {"total": 0, "members": {}}
            
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            # æ€»è®°å¿†æ•°
            cursor.execute("SELECT COUNT(*) FROM member_memories")
            total = cursor.fetchone()[0]
            
            # æŒ‰ç”¨æˆ·ç»Ÿè®¡
            cursor.execute("""
                SELECT user_id, COUNT(*) 
                FROM member_memories 
                GROUP BY user_id
            """)
            by_user = {row[0]: row[1] for row in cursor.fetchall()}
            
            conn.close()
            
            return {
                "total": total,
                "members": by_user,
                "last_updated": int(time.time())
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç¾¤ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total": 0, "error": str(e)}
    
    def get_all_stats(self) -> Dict[str, Any]:
        """è·å–å…¨å±€ç»Ÿè®¡"""
        try:
            # ç»Ÿè®¡ç§èŠæ•°æ®åº“æ•°é‡
            private_dbs = list(self.private_db_dir.glob("user_*.db"))
            
            # ç»Ÿè®¡ç¾¤èŠæ•°æ®åº“æ•°é‡
            group_dbs = list(self.group_db_dir.glob("group_*.db"))
            
            # ç»Ÿè®¡æ€»è®°å¿†æ•°
            total_private = 0
            total_group = 0
            
            for db_path in private_dbs:
                try:
                    conn = sqlite3.connect(str(db_path))
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM private_memories")
                    total_private += cursor.fetchone()[0]
                    cursor.execute("SELECT COUNT(*) FROM group_memories")
                    total_group += cursor.fetchone()[0]
                    conn.close()
                except Exception:
                    pass
            
            for db_path in group_dbs:
                try:
                    conn = sqlite3.connect(str(db_path))
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM member_memories")
                    total_group += cursor.fetchone()[0]
                    conn.close()
                except Exception:
                    pass
            
            return {
                "user_count": len(private_dbs),
                "group_count": len(group_dbs),
                "total_private_memories": total_private,
                "total_group_memories": total_group,
                "total_memories": total_private + total_group
            }
        except Exception as e:
            logger.error(f"âŒ è·å–å…¨å±€ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}


# å…¨å±€å•ä¾‹
_vector_service: Optional[FAISSVectorService] = None


def get_vector_service() -> FAISSVectorService:
    """è·å–å…¨å±€å‘é‡æœåŠ¡å•ä¾‹"""
    global _vector_service
    if _vector_service is None:
        _vector_service = FAISSVectorService()
    return _vector_service
