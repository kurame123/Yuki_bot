"""
çŸ¥è¯†åº“æ„å»ºå·¥å…· - FAISS + SQLite ç‰ˆæœ¬
è´Ÿè´£å°† knowledge_docs æ–‡ä»¶å¤¹ä¸­çš„æ–‡æœ¬æ–‡ä»¶åˆ‡ç‰‡ã€å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
    python tools/kb_builder/build_kb_faiss.py
"""
import sys
import os
import glob
import sqlite3
import numpy as np
import faiss
import pickle
from pathlib import Path

# è·¯å¾„é…ç½®
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from src.core.config_manager import ConfigManager
from src.services.vector_service import EmbeddingClient


class FAISSKBBuilder:
    """FAISS çŸ¥è¯†åº“æ„å»ºå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        ConfigManager.load()
        self.config = ConfigManager.get_bot_config()
        self.ai_config = ConfigManager.get_ai_config()
        
        # æ•°æ®åº“è·¯å¾„
        self.db_path = Path(project_root) / self.config.storage.vector_db_path
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        self.kb_db_path = self.db_path / "knowledge.db"
        self.kb_index_path = self.db_path / "knowledge.faiss"
        self.kb_id_map_path = self.db_path / "kb_id_map.pkl"
        
        self.vector_dim = self.ai_config.embedding.vector_dim
        
        # åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()
        
        print(f"âœ… æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®åº“è·¯å¾„: {self.kb_db_path}")
        print(f"   ç´¢å¼•è·¯å¾„: {self.kb_index_path}")
    
    def split_text(self, text: str, chunk_size: int = 150, overlap: int = 40) -> list:
        """æ™ºèƒ½æ–‡æœ¬åˆ‡ç‰‡"""
        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?', '\n\n']
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end < len(text):
                best_end = end
                for marker in sentence_endings:
                    search_start = start + chunk_size // 2
                    last_pos = text.rfind(marker, search_start, end + 10)
                    if last_pos != -1:
                        candidate_end = last_pos + len(marker)
                        if candidate_end > best_end - 20:
                            best_end = candidate_end
                end = best_end
            
            chunk = text[start:end].strip()
            if chunk and len(chunk) >= 10:
                chunks.append(chunk)
            
            if end > start + chunk_size - 10:
                start = end - overlap
            else:
                start = end
        
        return chunks
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºæ—§çŸ¥è¯†åº“"""
        try:
            print("ğŸ—‘ï¸  æ­£åœ¨æ¸…ç©ºæ—§çŸ¥è¯†åº“...")
            
            # åˆ é™¤æ—§æ–‡ä»¶
            if self.kb_db_path.exists():
                self.kb_db_path.unlink()
            if self.kb_index_path.exists():
                self.kb_index_path.unlink()
            if self.kb_id_map_path.exists():
                self.kb_id_map_path.unlink()
            
            print("âœ… æ—§çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç©ºçŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
    
    def run(self, clear_old: bool = True, use_cleaned: bool = True):
        """
        æ‰§è¡ŒçŸ¥è¯†åº“æ„å»º
        
        Args:
            clear_old: æ˜¯å¦æ¸…ç©ºæ—§çŸ¥è¯†åº“
            use_cleaned: æ˜¯å¦ä½¿ç”¨æ¸…æ´—åçš„ JSON æ•°æ®ï¼ˆæ¨èï¼‰
        """
        # 1. æ¸…ç©ºæ—§çŸ¥è¯†
        if clear_old:
            self.clear_knowledge_base()
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“
        conn = sqlite3.connect(str(self.kb_db_path))
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT NOT NULL,
                content TEXT NOT NULL,
                title TEXT,
                category TEXT
            )
        """)
        conn.commit()
        
        # 3. åˆå§‹åŒ– FAISS ç´¢å¼•
        index = faiss.IndexFlatIP(self.vector_dim)
        id_map = []
        
        # 4. é€‰æ‹©æ•°æ®æº
        if use_cleaned:
            # ä½¿ç”¨æ¸…æ´—åçš„ JSON æ•°æ®
            total_chunks = self._build_from_cleaned_json(conn, cursor, index, id_map)
        else:
            # ä½¿ç”¨åŸå§‹ txt æ–‡ä»¶
            total_chunks = self._build_from_raw_files(conn, cursor, index, id_map)
        
        # 5. ä¿å­˜æ•°æ®
        conn.commit()
        conn.close()
        
        # ä¿å­˜ FAISS ç´¢å¼•
        faiss.write_index(index, str(self.kb_index_path))
        
        # ä¿å­˜ ID æ˜ å°„
        with open(self.kb_id_map_path, 'wb') as f:
            pickle.dump(id_map, f)
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   âœ“ æ€»ç‰‡æ®µæ•°: {total_chunks}")
        print(f"   âœ“ ç´¢å¼•å¤§å°: {index.ntotal} æ¡")
        print(f"{'='*60}\n")
    
    def _build_from_cleaned_json(self, conn, cursor, index, id_map) -> int:
        """ä»æ¸…æ´—åçš„ JSON æ•°æ®æ„å»ºçŸ¥è¯†åº“"""
        import json
        
        json_file = project_root / "data" / "cleaned_knowledge.json"
        
        if not json_file.exists():
            print(f"âŒ æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            print(f"   è¯·å…ˆè¿è¡Œ: python tools/kb_cleaner.py")
            return 0
        
        print(f"ğŸ“– ä»æ¸…æ´—åçš„æ•°æ®æ„å»ºçŸ¥è¯†åº“: {json_file}")
        
        # è¯»å– JSON
        with open(json_file, 'r', encoding='utf-8') as f:
            metadata_list = json.load(f)
        
        print(f"âœ… åŠ è½½ {len(metadata_list)} æ¡å…ƒæ•°æ®")
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        print(f"\nğŸ’¾ æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å‚¨...")
        print(f"è¿›åº¦: ", end="", flush=True)
        
        for i, metadata in enumerate(metadata_list):
            title = metadata.get('title', '')
            content = metadata.get('content', '')
            source = metadata.get('source', 'unknown')
            
            # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
            full_text = f"{title}ï¼š{content}"
            
            # ç”Ÿæˆå‘é‡
            embedding = self.embedding_client.get_embedding(full_text)
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            
            # å­˜å‚¨å…ƒæ•°æ®åˆ° SQLite
            cursor.execute("""
                INSERT INTO knowledge (source, content, title)
                VALUES (?, ?, ?)
            """, (source, content, title))
            
            kb_id = cursor.lastrowid
            
            # æ·»åŠ å‘é‡åˆ° FAISS
            index.add(embedding.reshape(1, -1))
            id_map.append(kb_id)
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = ((i + 1) / len(metadata_list)) * 100
            bar_length = 30
            filled = int(bar_length * (i + 1) / len(metadata_list))
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
            print(f"\rè¿›åº¦: [{bar}] {progress:.1f}% ({i+1}/{len(metadata_list)})", end="", flush=True)
        
        print()
        print(f"âœ… å­˜å‚¨å®Œæˆï¼")
        
        return len(metadata_list)
    
    def _build_from_raw_files(self, conn, cursor, index, id_map) -> int:
        """ä»åŸå§‹ txt æ–‡ä»¶æ„å»ºçŸ¥è¯†åº“ï¼ˆæ—§æ¨¡å¼ï¼‰"""
        # 4. æ‰«ææ–‡æ¡£
        docs_dir = project_root / "knowledge_docs"
        if not docs_dir.exists():
            print(f"âŒ æ–‡æ¡£æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {docs_dir}")
            print(f"   è¯·åˆ›å»º knowledge_docs/ æ–‡ä»¶å¤¹å¹¶æ”¾å…¥ .txt æˆ– .md æ–‡ä»¶")
            return 0
        
        files = glob.glob(str(docs_dir / "*.txt")) + glob.glob(str(docs_dir / "*.md"))
        
        if not files:
            print("âŒ æœªå‘ç°ä»»ä½•æ–‡æ¡£")
            return 0
        
        print(f"\nğŸ“š å‘ç° {len(files)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹å¤„ç†...\n")
        
        # 5. å¤„ç†æ¯ä¸ªæ–‡ä»¶
        total_chunks = 0
        for file_idx, file_path in enumerate(files, 1):
            filename = os.path.basename(file_path)
            print(f"\n{'='*60}")
            print(f"ğŸ“„ [{file_idx}/{len(files)}] æ­£åœ¨å¤„ç†: {filename}")
            print(f"{'='*60}")
            
            try:
                # è¯»å–æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not content.strip():
                    print(f"âš ï¸  æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡\n")
                    continue
                
                print(f"âœ… æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                
                # æ–‡æœ¬åˆ‡ç‰‡
                print(f"âœ‚ï¸  æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
                chunks = self.split_text(content)
                print(f"âœ… åˆ‡åˆ†å®Œæˆ: {len(chunks)} ä¸ªç‰‡æ®µ")
                
                # å‘é‡åŒ–å¹¶å­˜å‚¨
                print(f"\nğŸ’¾ æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å‚¨...")
                print(f"è¿›åº¦: ", end="", flush=True)
                
                for i, chunk in enumerate(chunks):
                    # ç”Ÿæˆå‘é‡
                    embedding = self.embedding_client.get_embedding(chunk)
                    # å½’ä¸€åŒ–
                    norm = np.linalg.norm(embedding)
                    if norm > 0:
                        embedding = embedding / norm
                    
                    # å­˜å‚¨å…ƒæ•°æ®åˆ° SQLite
                    cursor.execute("""
                        INSERT INTO knowledge (source, content, title)
                        VALUES (?, ?, ?)
                    """, (filename, chunk, filename.rsplit('.', 1)[0]))
                    
                    kb_id = cursor.lastrowid
                    
                    # æ·»åŠ å‘é‡åˆ° FAISS
                    index.add(embedding.reshape(1, -1))
                    id_map.append(kb_id)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = ((i + 1) / len(chunks)) * 100
                    bar_length = 30
                    filled = int(bar_length * (i + 1) / len(chunks))
                    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                    print(f"\rè¿›åº¦: [{bar}] {progress:.1f}% ({i+1}/{len(chunks)})", end="", flush=True)
                
                print()
                total_chunks += len(chunks)
                print(f"âœ… å­˜å‚¨å®Œæˆï¼")
                
            except Exception as e:
                print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return total_chunks


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ§  çŸ¥è¯†åº“æ„å»ºå·¥å…· v2.0 (FAISS + SQLite)")
    print("="*60 + "\n")
    
    try:
        builder = FAISSKBBuilder()
        
        print("ğŸ“‹ è¯·é€‰æ‹©æ•°æ®æºï¼š")
        print("   [1] ä½¿ç”¨æ¸…æ´—åçš„ JSON æ•°æ®ï¼ˆæ¨èï¼Œæ›´æ¸…æ™°ï¼‰")
        print("   [2] ä½¿ç”¨åŸå§‹ txt æ–‡ä»¶ï¼ˆæ—§æ¨¡å¼ï¼‰")
        source_choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
        
        use_cleaned = (source_choice == '1')
        
        print("\nâš ï¸  æ˜¯å¦æ¸…ç©ºæ—§çŸ¥è¯†åº“ï¼Ÿ")
        print("   [Y] æ˜¯ï¼ˆå…¨é‡æ›´æ–°ï¼Œåˆ é™¤æ—§æ•°æ®ï¼‰")
        print("   [N] å¦ï¼ˆå¢é‡æ›´æ–°ï¼Œä¿ç•™æ—§æ•°æ®ï¼‰")
        clear_choice = input("\nè¯·é€‰æ‹© (Y/N): ").strip().upper()
        
        clear_old = (clear_choice == 'Y')
        
        print()
        builder.run(clear_old=clear_old, use_cleaned=use_cleaned)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
